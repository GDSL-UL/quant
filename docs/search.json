[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Human Geography through Merseyside - Quantitative Block: Seeing the world through numbers",
    "section": "",
    "text": "Welcome\nThis is the website for “Human Geography through Merseyside - Quantitative Block: Seeing the world through numbers” (module ENVS162) at the University of Liverpool. This block of the module is designed and delivered by Dr. Zi Ye and Dr. Ron Mahabir from the Geographic Data Science Lab at the University of Liverpool. The module seeks to provide hands-on experience and training in introductory statistics for human geographers.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Human Geography through Merseyside - Quantitative Block: Seeing the world through numbers",
    "section": "Contact",
    "text": "Contact\n\nZi Ye - zi.ye [at] liverpool.ac.uk Lecturer in Geographic Information Science Office 107, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nRon Mahabir - Ron.Mahabir [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 4xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "general/overview.html",
    "href": "general/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aim and Learning Objectives\nThis sub-module aims to provide training and skills on a set of basic quantitative skills for data collection, analysis, and interpretation and to enable you to link conceptual ideas with real world examples. This block serves as the foundation for Year 2 BA field class and, optionally, for Year 3 dissertation.\nBackground\nData and research are key pillars of the global economy and society today. We need rigorous approaches to collecting and analysing both the statistics that can tell us ‘how much’ and if there are observable relationships between phenomena; and the information gives us a nuanced understanding of cultural contexts and human dynamics. Quantitative skills enable us to explore and measure socio-economic activities and processes at large scales, while qualitative skills enable understanding of social, cultural, and political contexts and diverse lived experiences. Rather than being in opposition, qualitative and quantitative research can complement one another in the investigation of today’s pressing research questions.\nTo these ends, this block will help you develop your quantitative skills, as critical tools. This course will help you understand what quantitative statistical researchers use and develop a set of research techniques that can be used in your field classes and dissertations.\nLearning objectives:",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#aim-and-learning-objectives",
    "href": "general/overview.html#aim-and-learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Understand how to explore a dataset, containing a number of observations described by a set of variables.\nDemonstrate an understanding in the application and interpretation of commonly used quantitative research methods.\nAbility to work with quantitative data to understand real-world social phenomenian and patterns.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#module-structure",
    "href": "general/overview.html#module-structure",
    "title": "Overview",
    "section": "Module Structure",
    "text": "Module Structure\nStaff: Dr Zi Ye and Dr Ron Mahabir\nWhere and When\nWeek 1 - 5 Lecture: Tuesday (12am – 1pm) @ Mathematical Sciences, Proudman Lecture Theatre\nWeek 1 - 6 Practical PC session: Friday (9 – 11 am) @ Central Teaching Lab: PC Teaching Centre\nLectures will introduce and explain the fundamentals of quantitative methods, with the opportunity to apply the method introduced in the labs later in the week.\nThe computer practical sessions, will give you the chance to use and apply quantitative methods to real-world data. These are primarily self-directed sessions, but with support on hand if you get stuck. Support and training in R will be provided through these sessions. Weekly sessions will be driven by empirical research questions.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nFormat\nStaff\n\n\n\n\n1\nIntroduction\nGetting Started in RStudio: Knowing Merseyside\nLecture\nComputer Lab Practical\nZY/RM\n\n\n2\nExploratory Data Analysis: UK Election\nLecture and Computer Lab Practical\nZY\n\n\n3\nSampling and data manipulation: Happiness around the world\nLecture and Computer Lab Practical\nZY\n\n\n4\nCorrelation, data reliability and the issue of scale: Health\nLecture and Computer Lab Practical\nRM\n\n\n5\nHow robust are my findings\nLecture and Computer Lab Practical\nRM\n\n\n6\nOnline Assessment\nComputer Lab\nRM/ZY",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#software-and-data",
    "href": "general/overview.html#software-and-data",
    "title": "Overview",
    "section": "Software and Data",
    "text": "Software and Data\nFor quantitative training sessions, ensure you have installed and/or have access to RStudio. To run the analysis and reproduce the code in R, you need the following software installed on your machine:\n\nR-4.2.2 (or later)\nRStudio 2022.12.0-353 (or later)\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN).\nRStudio, download the appropriate version from here.\n\nThis software is already installed on University Machines. But you will need it to run the analysis on your personal devices.\nData\nExample datasets could be accessed through ENVS 162 Canvas module every week.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/assessment.html",
    "href": "general/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "Week 6 Computer-based ‘open book’ multiple-choice exam\n\nThe online assessment will be released at 4pm on Thursday 5th March and should be completed by 4pm on Friday 6th March.\nAlso available 06/03/2025 9:00 – 11:00 CTL PC Teaching Centre (1st Floor CTL)\nShould take less 90 minutes; c. 20 questions; 24 hours to complete\nQuestions and answers randomised for each student (anti-cheating measure)\nSome questions of factual recall, more requiring data analysis to find answers\n\nPreparation for assessment\n\nWeekly lecture & weekly computer practical ‘clinic sessions’\nWeekly holding hands formative tasks at the last 20 mins of the practical session\nWeek 5 mock online test",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html",
    "href": "labs/01.GettingStartedinRStudio.html",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "",
    "text": "1.1 Overview\nThis practical intend to prepare students who have limited experiences with R and RStudio. The content are adapted based on",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html#overview",
    "href": "labs/01.GettingStartedinRStudio.html#overview",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "",
    "text": "Brunsdon, Chris, and Lex Comber. 2018. An Introduction to r for Spatial Analysis and Mapping (2e). Sage.\nComber, Lex, and Chris Brunsdon. 2021. Geographical Data Science and Spatial Data Analysis: An Introduction in r. Sage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html#getting-set-up-with-rstudio",
    "href": "labs/01.GettingStartedinRStudio.html#getting-set-up-with-rstudio",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "1.2 Getting set up with RStudio",
    "text": "1.2 Getting set up with RStudio\n\n1.2.1 Install R and RStudio (if necessary)\nR is a free, open-source programming language used for statistical analysis, data visualization, and data science\nRStudio is a free front-end to R, designed to make using R easier\nAll of the PCs in the University PC Teaching Centre used for this class come with R and RStudio pre-installed, as do the PCs in many other University PC Teaching Centres.\nHowever, you may wish to install R and RStudio on your own computer, or on a University PC that lacks them.\nUniversity computers: Use the Install University Applications app on the computer to install the latest version of RStudio (this will also install the latest version of R)\nYour own computer: R and RStudio can be downloaded from the CRAN website and installed your own computer - see below for details. A key point is that you should install R before you install RStudio.\nThe simplest way to get R installed on your computer is to go the download pages on the R website - a quick search for `download R’ should take you there, but if not you could try:\n\nWindows: https://cran.r-project.org/bin/windows/base/\nMac: https://cran.r-project.org/bin/macosx/\nLinux: http://cran.r-project.org/bin/linux/\n\nThe Windows and Mac version come with installer packages and are easy to install whilst the Linux binaries require use of a command terminal.\nRStudio can be downloaded from https://www.rstudio.com/products/rstudio/download/ and the free version of RStudio Desktop is more than sufficient for this module and all the other things you will to do at degree level.\nIf you experience any problems installing R or RStudio on your own computer, bring it to one of the class lab sessions where we will be able to provide advice.\n\n\n1.2.2 File management\nBefore you start installing software or downloading data, create a folder on your M-Drive (if working on a University networked machine) or locally if working on your own device – name this ‘ENVS162’ and within this create a sub-folder for each practical session. For this session, create a sub-folder called Week1 in your ENVS162 folder on your M-Drive. Take care to ensure you do not delete any work you do complete in the practical sessions. It is imperative that you practice good file management!\n\n\n1.2.3 Open RStudio\nRStudio provides an interface to the different things that R can do via the 4 panes: the Console where code is entered (bottom left), a Source pane with R scripts (top left), the variables in the working Environment (top right), Files, Plots, Help etc (bottom right) - see the RStudio environment in Figure below.\nIn the figure above of the RStudio interface, a new script has been opened, a line of code had been written and then run in the console. The code assigns a value of 100 to x. The file has been saved into the current working environment. You are expected to define a similar set up for each practical as you work through the code. Note that in the script, anything that follows a # is a comment and ignored by R.\nUsers can set up their personal preferences for how they like their RStudio interface. Similar to straight R, there are very few pull-down menus in R, and therefore you will type lines of code into your script and run these in what is termed a command line interface (the console). Like all command line interfaces, the learning curve is steep but the interaction with the software is more detailed which allows greater flexibility and precision in the specification of commands.\nBeyond this there are further choices to be made. Commands can be entered in two forms: directly into the R console window or as a series of commands into a script window. We strongly advise that all code should be written in a script - (a .R file) and then run from the script. - To run code in a script, place the cursor on the line of code and then run by pressing the ‘Run’ icon at the top left of the script pane, or by pressing Ctrl Enter (PC) (or Cmd Enter on a Mac).\n\n\n\n1.2.4 Ways of working\nThe first set of consideration relate to how you should work in R/RStudio. The key things to remember are:\n\nR is a learning curve if you have never done anything like this before. It can be scary. It can be intimidating. But once you have a bit of familiarity with how things work, it is incredibly powerful.\nYou will be working from practical worksheets which will have all the code you need. Your job is to try to understand what the code is doing and not to remember the code. Comments in your code really help.\nTo help you do this, the very strong suggestion is use the R scripts that are provided, and that you add your own comments to help you understand what is going on when you return to them. Comments are prefaced by a hash (#) that is ignored by R. Then you can save your code (with comments), run it and return to it later and modify at your leisure.\n\nThe module places a strong emphasis placed on learning by doing, which means that you encouraged to unpick the code that you are given, adapt it and play with it. It is not about remembering or being able to recall each function used but about understanding what is being done. If you can remember what you did previously (i.e. the operations you undertook) and understand what you did, you will be able to return to your code the next time you want to do something similar. To help you with this you should:\n\nAlways run your code from an R script… always! These are provided for each practical;\nAnnotate you scripts with comments. These are prefixed by a hash (#) in the code;\nSave your R script to your folder.\n\n\nIn Summary:\n\nYou should always use a script (a text file containing code) for your code which can be saved and then re-run at a later date.\nYou can write your own code into a script, copy and paste code into it or use an existing script (for example as provided for each of the R/RStudio practicals in this module).\nTo open a new R script go to File &gt; New File &gt; R Script to open a new R file, and save it with a sensible name.\nTo load an existing script file go to File &gt; Open File and then navigate to your file. Or, if you have recently opened the file, go to File &gt; Recent Files &gt;.\nIt is good practice to set the working directory at the beginning of your R session. This can be done via the menu in RStudio Session &gt; Set Working Directory &gt; …. This points the R session to the folder you choose and will ensure that any files you wish to read, write or save are placed in this directory.\nTo run code in a script, place the cursor on the line of code and then run by pressing the ‘Run’ icon at the top left of the script pane, or by pressing Ctrl Enter (PC) or Cmd Enter (Mac).\n\n\n\n\n1.2.5 Your first R code\nIn this section you will undertake a few generic operations. You will:\n\nundertake assignment: the allocation of values to an R object.\nuse assignment to create a vector of elements and a matrix of elements.\nundertake operations on R objects.\napply some functions to R objects (functions nearly always return a value).\naccess some of R in-built data to examine a data table (or data.frame which is like an Excel spreadsheet).\ndo some basic plotting, including scatter plots and histograms.\ncreate data summaries.\n\nOn the way you will also be introduced to indexing.\nFirst, you should create a new R script (see above) and save it as week1.R in the working directory you are using for this practical. This should be the Week1 sub-directory you created in the ENVS162 folder. Note that you should create a separate folder for each week’s practical.\n\n1.2.5.1 Assignment\nThe command line prompt in the Console window, the &gt;, is an invitation to start typing in your commands.\nWrite the following into your script: 3+5 and run it. Recall that code is run done by either by pressing the Run icon at the top left of the script pane, or by pressing Ctrl Enter (PC) or Cmd Enter (Mac).\n\n3+5\n\n[1] 8\n\n\nHere the result is 8. The [1] that precedes the output it formally indicates, first requested element will follow. In this case there is just one element. The &gt; indicates that R is ready for another command.\nNow type the following in to your script and run it:\n\ny &lt;- 3+5\ny\n\n[1] 8\n\n\nHere the value of the 3+5 has been assigned to y. The syntax y &lt;- 3+5 can be read as y gets 3+5. When y is invoked its value is returned (8).\nFor the purposes of this module, in R the equals sign (=) is the same as &lt;-, a left diamond bracket &lt; followed by a minus sign -. This too is interpreted by R as is assigned to or gets when the code is read right to left.\nNow copy and paste the following into your R script and run both lines:\n\nx &lt;- matrix(c(1,2,3,4,5,6,7,8), nrow = 4)\ny = matrix(1:8, nrow = 4, byrow = T)\n\nYou should see the x appear with the y in the Environment pane. y has now been overwritten with a new assignment. If you click on the icon next to them, you will get a ‘spreadsheet’ view of the data you have created.\nOf course you can also enter the following in the console and see what is returned:\n\nx\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\ny\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n[4,]    7    8\n\n\nNote In the code snippets above you have used parentheses - round brackets. Different kinds of brackets are used in different ways in R. Parentheses are used with functions, and contain the arguments that are passed to the function, separated by commas (,).\nIn this case the functions are c() and matrix(). The function c() combines or concatenates elements into a vector, and matrix() creates a matrix of elements in a tabular format.\nIn the line of code x = matrix(c(1,2,3,4,5,6,7,8), nrow = 4), the arguments passed to the matrix() function are the vector of values c(1,2,3,4,5,6,7,8) and nrow = 4. Other kinds of brackets are used in different ways as you will see later.\nOne final thing to note is that the matrix is y is has the numbers 1 to 8, but this is specified by 1:8. Try entering 3:65, 19:11, and 1.5:5 to see how the colon (:) works in this context.\n\n\n1.2.5.2 Operations\nNow you can undertake operations on R objects and apply functions to them. Write the following code into your script and then run it:\n\n# x is a matrix\nx\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n# multiplication\nx*2\n\n     [,1] [,2]\n[1,]    2   10\n[2,]    4   12\n[3,]    6   14\n[4,]    8   16\n\n# sum of x\nsum(x)\n\n[1] 36\n\n# mean of x\nmean(x)\n\n[1] 4.5\n\n\nOperations can be undertaken directly using mathematical notation like * for multiplication or using functions like max to find the maximum value in an R object.\n\n\n1.2.5.3 Functions\nFunctions are always followed by parenthesis (round brackets) ( ). These are different from square and curly brackets [ ] and { }. Functions always return something, a result if you like, and have the generic form:\n\n# don't run this or write this into your script!\nresult = function(value or R object, other parameters)\n\nDo not run or enter this code in your script - it is an example!\n\n\n1.2.5.4 Data Tables\nHere we will load a data table in data.frame (like a spreadsheet) in R/RStudio. R has number of in-built datasets that we can use the code below loads one of these:\n\ndata(mtcars)\nclass(mtcars)\n\n[1] \"data.frame\"\n\n\nHave a look at what is loaded by listing the objects in the current R session\n\nls()\n\n[1] \"mtcars\" \"x\"      \"y\"     \n\n\nYou should see the mtcars object. You can examine this data in a number of ways\n\n# the structure of mtcars\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n# the first six rows (or head) of mtcars\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThe mtcars object is a data.frame, a kind of data table, and it has a number of attributes which are all numeric. The code below prints it all out to the console:\n\nmtcars\n\nData frames are ‘flat’ in that they typically have a rectangular layout like a spreadsheet, with rows typically relating to observations (individuals, areas, people, houses, etc) and columns relating to their properties or attributes (height, age, etc). The columns in data frames can be of different types: vectors of numbers, factors (classes) or text strings. In matrices all of the columns have to be off the same type. Data frames are central to what we will do in R.\n\n\n1.2.5.5 Plotting the data: ‘Hello World!’\nThe code below creates a plot of 2 variables counts in the data: mpg and disp.\n\nplot(disp ~ mpg,  data = mtcars, pch=16)\n\n\n\n\n\n\n\n\nThe option pch=16 sets the plotting character to a solid black dot. More plot characters are available - examine the help for points() to see these (For any command, if you are the first time use it, you can always ask R to explain to you by using ? as help)\n\n?points\n\nThis plot can be improved greatly. We can specify more informative axis labels, change size of the text and of the plotting symbol, and so on.\nWe can also specify the same plot by passing named variables to the plot function directly as well as other parameters, as in the figure. Notice how the syntax for this is different to the plot function above, and the different parameters that are passed to the plot() function:\n\nplot(x = mtcars$mpg, y = mtcars$disp,   pch = 1, col = \"dodgerblue\", \n     cex = 1.5, xlab = \"Miles per Gallon\", ylab = \"Displacement\", \n     main = \"Hello World!\")\n\n\n\n\n\n\n\n\nNotice how the dollar sign ($) is used to access variables in the mtcars data table compared to the first plot command, which specified data = mtcars.\n\n\n1.2.5.6 Data summaries and indexing\nWe may for example require information on variables in mtcars. The summary function is very useful:\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nThis shows different summaries of the individual attributes in mtcars.\nThe main R graphics function is plot(). In addition to plot() there are functions for adding points and lines to existing graphs, for placing text at specified positions, for specifying tick marks and tick labels, for labelling axes, and so on.\nThere are various other alternative helpful forms of graphical summary. A helpful graphical summary for the mtcars data frame is the scatterplot matrix.\n\n# return the names of the mtcars variables\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n# return the 3rd to 7th names\nnames(mtcars)[c(3:7)]\n\n[1] \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\"\n\n\n\n# check what this does\nc(3:7)\n\n[1] 3 4 5 6 7\n\n\n\n# plot the 3rd to 7th variables in mtcars\nplot(mtcars[, c(3:7)], cex = 0.5, \n     col = \"red\", upper.panel=panel.smooth)\n\n\n\n\n\n\n\n\nThe results show the correlations between the variables in the mtcars data frame, and the trend of their relationship is included with the upper.panel=panel.smooth parameter passed to plot.\nThere are number of things to notice here (as well as the figure). In particular note the use of the vector c(2:7) to subset the columns of mtcars:\n\nIn the second line, this is was used to subset the vector of column names created by names(mtcars).\nIn the third line, it was printed out. Notice how 3:7 printed out all the number between 3 and 7 - very useful.\nFor the plot, the vector was passed to the second argument, after the comma, in the square brackets [,] to indicate which columns were to be plotted.\n\nThe referencing in this way (or indexing) is very important: the individual rows and columns of 2 dimensional data structures like data frames, matrices, tibbles etc can be accessed by passing references to them in the square brackets.\n\n# 1st row\nmtcars[1,]\n\n          mpg cyl disp  hp drat   wt  qsec vs am gear carb\nMazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n\n# 3rd column\nmtcars[,3]\n\n [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0 146.7 140.8 167.6 167.6 275.8\n[13] 275.8 275.8 472.0 460.0 440.0  78.7  75.7  71.1 120.1 318.0 304.0 350.0\n[25] 400.0  79.0 120.3  95.1 351.0 145.0 301.0 121.0\n\n# a selection of rows\nmtcars[c(3:5,8),]\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\n\nSuch indexing could of course have been assigned to a R object and used to do the subsetting:\n\nx = c(3:7)\nnames(mtcars)[x]\n\n[1] \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\"\n\n\nThus indexing allows specific rows and columns to be extracted from the data as required.\nNote You have encountered a second type of brackets, square brackets [ ]. These are used to reference or index positions in a vector or a data table.\nConsider the object x above. It contains a vector of values, 3,4,5,6,7. Entering x[1] would extract the first element of x, in this case 3. Similarly x[4] would return the 4th element and x[c(1,4)] would return the 1st and 4th elements of x.\nHowever, in the examples above that index the 2-dimensional mtcars object, the square brackets are used to index row and column positions. The syntax for this is [rows, columns]. We will be using such indexing throughout this module.\nYou can ask R to return you specific rows and columns by different ways:\n\nmtcars[c(2,9), 3:7]\n\n               disp  hp drat    wt  qsec\nMazda RX4 Wag 160.0 110 3.90 2.875 17.02\nMerc 230      140.8  95 3.92 3.150 22.90\n\nmtcars[3:6, c(\"disp\",\"hp\",\"qsec\")]\n\n                  disp  hp  qsec\nDatsun 710         108  93 18.61\nHornet 4 Drive     258 110 19.44\nHornet Sportabout  360 175 17.02\nValiant            225 105 20.22\n\nmtcars [, c(\"wt\",\"gear\",\"cyl\")]\n\n                       wt gear cyl\nMazda RX4           2.620    4   6\nMazda RX4 Wag       2.875    4   6\nDatsun 710          2.320    4   4\nHornet 4 Drive      3.215    3   6\nHornet Sportabout   3.440    3   8\nValiant             3.460    3   6\nDuster 360          3.570    3   8\nMerc 240D           3.190    4   4\nMerc 230            3.150    4   4\nMerc 280            3.440    4   6\nMerc 280C           3.440    4   6\nMerc 450SE          4.070    3   8\nMerc 450SL          3.730    3   8\nMerc 450SLC         3.780    3   8\nCadillac Fleetwood  5.250    3   8\nLincoln Continental 5.424    3   8\nChrysler Imperial   5.345    3   8\nFiat 128            2.200    4   4\nHonda Civic         1.615    4   4\nToyota Corolla      1.835    4   4\nToyota Corona       2.465    3   4\nDodge Challenger    3.520    3   8\nAMC Javelin         3.435    3   8\nCamaro Z28          3.840    3   8\nPontiac Firebird    3.845    3   8\nFiat X1-9           1.935    4   4\nPorsche 914-2       2.140    5   4\nLotus Europa        1.513    5   4\nFord Pantera L      3.170    5   8\nFerrari Dino        2.770    5   6\nMaserati Bora       3.570    5   8\nVolvo 142E          2.780    4   4\n\n\n\n\n1.2.5.7 Packages\nThe base installation of R includes many functions and commands. However, more often we are interested in using some particular functionality, encoded into packages contributed by the R developer community. Installing packages for the first time can be done at the command line in the R console using the install.packages command as in the example below to install the tmap library or via the RStudio menu via Tools &gt; Install Packages.\nWhen you install these packages it is strongly suggested you also install the dependencies. These are other packages that are required by the package that is being installed. This can be done by selecting check the box in the menu or including dep=TRUE in the command line as below (don’t run this yet!):\n\n# don't run this!\ninstall.packages(\"tidyverse\", dep = TRUE)\n\nYou may have to set a mirror site from which the packages will be downloaded to your computer. Generally you should pick one that is nearby to you.\nFurther descriptions of packages, their installation and their data structures will be given as needed in the practicals. There are literally 1000s of packages that have been contributed to the R project by various researchers and organisations. These can be located by name at http://cran.r-project.org/web/packages/available_packages_by_name.html if you know the package you wish to use. It is also possible to search the CRAN website to find packages to perform particular tasks at http://www.r-project.org/search.html. Additionally many packages include user guides and vignettes as well as a PDF document describing the package and listed at the top of the index page of the help files for the package.\nAs well as tidyverse you should install the sf package and dependencies. So we have 2 packages to install:\n\nsf for spatial data and spatial objects\ntidyverse for lots of lovely data science things - see https://www.tidyverse.org\n\nYou could do this in one go and this will take a bit of time:\n\ninstall.packages(c(\"sf\", \"tidyverse\"), dep = TRUE)\n\nRemember: you will only have to install a package once!! So when the above code has run in your script you should comment it out. For example you might want to include something like the below in your R script.\n\n# packages only need to be loaded once\n# install.packages(c(\"sf\", \"tidyverse\"), dep = TRUE)\n\nOnce the package has been installed on your computer then the package can be called using the library() function into each of your R sessions as below.\n\nlibrary(tidyverse)\nlibrary(sf)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html#knowing-merseyside",
    "href": "labs/01.GettingStartedinRStudio.html#knowing-merseyside",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "1.3 Knowing Merseyside",
    "text": "1.3 Knowing Merseyside\n\n1.3.1 Merseyside districts\nNow we use these basic R command and newly installed packages to start our initial exploration by using some existing secondary dataset from the Census 2021.\nIn R we normally read in tabular dataset from .csv format. In your ENVS162 Canvas page find Week 1 -&gt; Practical 1 Dataset, download the four datasets to your current working folder on your M drive (ENVS162 - Week 1). You may first identify one .csv dataset: merseyside.csv. You can open them in excel to have a look, but here we are using R instead of Excel to load and examine them.\n\n1.3.1.1 Loading tabular data\nThe survey data can be loaded into RStudio using the read.csv function.\nHowever, you will need to tell R where to get the data from. The easiest way to do this is to use the menu if the R script file is open. Go to Session &gt; Set Working Directory &gt; To Source File Location to set the working directory to the location where your week1.R script is saved. When you do this you will see line of code print out in the Console (bottom left pane) similar to setwd(\"SomeFilePath\"). You can copy this line of code to your script and paste into the line above the line calling the read.csv function.\n\n# use read.csv to load a CSV file\n# this is assignment to an object called `df`\ndf = read.csv(file = \"merseyside.csv\", stringsAsFactors = TRUE)\n\nThe stringsAsFactors = TRUE parameter tells R to read any character or text variables as classes or categories and not as just text.\nYou could inspect the help for the read.csv function to see the different parameters and their default values:\n\nhelp(read.csv)\n# or \n?read.csv\n\nFunctions always return something and in this case read.csv() function has returned a tabular R object with 5 records and 12 fields. This has been assigned to df.\nFinally in this section, lets have a look at the data. This can be done in a number of ways.\n\nyou could look at the df object by entering df in the Console. However this is not particular helpful as it simply prints out everything that is in df to the Console.\nyou could click on the df object in the Environment pane and this shows the structure of the attributes in different fields.\nyou could click on the little grid-like icon next df in the Environment pane to get a View of the data and remember to close the tab that opens!.\nor you could use some code as in the examples below.\n\nFirst, let’s have a look at the internal structure of the data using the str function:\n\nstr(df)\n\n'data.frame':   5 obs. of  12 variables:\n $ LAD21CD           : Factor w/ 5 levels \"E08000011\",\"E08000012\",..: 1 2 3 4 5\n $ District          : Factor w/ 5 levels \"Knowsley\",\"Liverpool\",..: 1 2 4 3 5\n $ Population        : int  154519 486089 183248 279234 320196\n $ Households        : int  66073 207491 81011 123075 143253\n $ Working_population: int  69495 205749 82622 124596 139500\n $ Students          : int  7050 59628 7582 12636 14642\n $ Unemployed        : int  3852 13894 4076 6143 6542\n $ Age_over_65       : int  26242 74322 37642 64763 70391\n $ Disability        : int  34990 105962 40829 61134 73088\n $ No_central_heating: int  1020 4822 1003 1965 2125\n $ Overcrowding      : int  1892 7352 1888 2700 2355\n $ Working_from_home : int  14880 53721 18973 34750 37299\n\n\nThere is other ways to get info about the number of rows and columns:\n\nnrow(df)\n\n[1] 5\n\nncol(df)\n\n[1] 12\n\n#or both row and col\ndim(df)\n\n[1]  5 12\n\n\nThe head function does this by printing out the first six records of the data table and you may need to scroll up and down in the Console pane to see all of what is returned.\n\nhead(df)\n\n    LAD21CD   District Population Households Working_population Students\n1 E08000011   Knowsley     154519      66073              69495     7050\n2 E08000012  Liverpool     486089     207491             205749    59628\n3 E08000013 St. Helens     183248      81011              82622     7582\n4 E08000014     Sefton     279234     123075             124596    12636\n5 E08000015     Wirral     320196     143253             139500    14642\n  Unemployed Age_over_65 Disability No_central_heating Overcrowding\n1       3852       26242      34990               1020         1892\n2      13894       74322     105962               4822         7352\n3       4076       37642      40829               1003         1888\n4       6143       64763      61134               1965         2700\n5       6542       70391      73088               2125         2355\n  Working_from_home\n1             14880\n2             53721\n3             18973\n4             34750\n5             37299\n\n\nAnother way to explore the data is through the summary function:\n\nsummary(df)\n\n      LAD21CD        District   Population       Households    \n E08000011:1   Knowsley  :1   Min.   :154519   Min.   : 66073  \n E08000012:1   Liverpool :1   1st Qu.:183248   1st Qu.: 81011  \n E08000013:1   Sefton    :1   Median :279234   Median :123075  \n E08000014:1   St. Helens:1   Mean   :284657   Mean   :124181  \n E08000015:1   Wirral    :1   3rd Qu.:320196   3rd Qu.:143253  \n                              Max.   :486089   Max.   :207491  \n Working_population    Students       Unemployed     Age_over_65   \n Min.   : 69495     Min.   : 7050   Min.   : 3852   Min.   :26242  \n 1st Qu.: 82622     1st Qu.: 7582   1st Qu.: 4076   1st Qu.:37642  \n Median :124596     Median :12636   Median : 6143   Median :64763  \n Mean   :124392     Mean   :20308   Mean   : 6901   Mean   :54672  \n 3rd Qu.:139500     3rd Qu.:14642   3rd Qu.: 6542   3rd Qu.:70391  \n Max.   :205749     Max.   :59628   Max.   :13894   Max.   :74322  \n   Disability     No_central_heating  Overcrowding  Working_from_home\n Min.   : 34990   Min.   :1003       Min.   :1888   Min.   :14880    \n 1st Qu.: 40829   1st Qu.:1020       1st Qu.:1892   1st Qu.:18973    \n Median : 61134   Median :1965       Median :2355   Median :34750    \n Mean   : 63201   Mean   :2187       Mean   :3237   Mean   :31925    \n 3rd Qu.: 73088   3rd Qu.:2125       3rd Qu.:2700   3rd Qu.:37299    \n Max.   :105962   Max.   :4822       Max.   :7352   Max.   :53721    \n\n\nFinally in this section, we come back to the dollar sign ($). This is used to refer to or extract an individual named field or variable in an R object, like df.\nThe code below prints out the Population attribute and generates a summary of its values:\n\n# extract an individual variable\ndf$Population\n\n[1] 154519 486089 183248 279234 320196\n\n\n\n# generate a summary of an individual variable\nsummary(df$Population)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 154519  183248  279234  284657  320196  486089 \n\n\nAnd of course we can use such operations to assign the result to new R objects. The code below extracts three variables from df, assigns them to x, y and z, and then uses the data.frame function to convert these into a new data.frame object called my_df\n\n# extract three variables, assigning them to temporary R objects\nx = df$District\ny = df$Working_population\nz = df$Students\n# create a data.frame from these, naming the new variables\nmy_df = data.frame(district = x,worker = y,student = z)\n\nYou should have a look at what you have created:\n\nhead(my_df)\n\n    district worker student\n1   Knowsley  69495    7050\n2  Liverpool 205749   59628\n3 St. Helens  82622    7582\n4     Sefton 124596   12636\n5     Wirral 139500   14642\n\n\n\nsummary(my_df)\n\n       district     worker          student     \n Knowsley  :1   Min.   : 69495   Min.   : 7050  \n Liverpool :1   1st Qu.: 82622   1st Qu.: 7582  \n Sefton    :1   Median :124596   Median :12636  \n St. Helens:1   Mean   :124392   Mean   :20308  \n Wirral    :1   3rd Qu.:139500   3rd Qu.:14642  \n                Max.   :205749   Max.   :59628  \n\n\nThe temporary R objects can be removed from the Environment using the rm function and a combine vector function, c() that you encountered in Week 19, that takes a vector of object names (hence they are in quotes) as its arguments.\n\nrm(list = c(\"x\",\"y\",\"z\"))\n\n\n\n1.3.1.2 Basic data manipulation\nNow we can do some basic data manipulation to know Merseyside more from the data perspective.\nWhat is the total population in Merseyside?\n\nsum(df$Population)\n\n[1] 1423286\n\n\nWhat is the total number of full-time students in Merseyside?\n\nsum(df$Students)\n\n[1] 101538\n\n\nThen, we can calculate the total number of workers that working from home:\n\nsum(df$Working_from_home)\n\n[1] 159623\n\n\nWhat is the proportion of working population actually work from home in Merseyside? Yes, we need to use a division calculation of the total number of working from home vs. all the working population. R can do it by:\n\nsum(df$Working_from_home) / sum(df$Working_population)\n\n[1] 0.2566443\n\n\nSo the answer is 25.7% for the whole Merseyside - but which district has the highest proportion and which as the lowest? You may have your own guessing. But let R do the calculation:\n\ndf$Prop.WFH = df$Working_from_home / df$Working_population * 100 #add a new column called Prop.WFH\ndf #print out the df\n\n    LAD21CD   District Population Households Working_population Students\n1 E08000011   Knowsley     154519      66073              69495     7050\n2 E08000012  Liverpool     486089     207491             205749    59628\n3 E08000013 St. Helens     183248      81011              82622     7582\n4 E08000014     Sefton     279234     123075             124596    12636\n5 E08000015     Wirral     320196     143253             139500    14642\n  Unemployed Age_over_65 Disability No_central_heating Overcrowding\n1       3852       26242      34990               1020         1892\n2      13894       74322     105962               4822         7352\n3       4076       37642      40829               1003         1888\n4       6143       64763      61134               1965         2700\n5       6542       70391      73088               2125         2355\n  Working_from_home Prop.WFH\n1             14880 21.41161\n2             53721 26.10997\n3             18973 22.96362\n4             34750 27.89014\n5             37299 26.73763\n\n\nHere we ask R to add a new column named Prop.WFH which is the working from home proportion that calculated by the number of working from home people in each district divided by the total working population in that district. The * 100 convert the rate in the percentage number. R will automatically do it row-by-row. We then print out the df, you may find at the very right end of the tabular, there is a new column called Prop.WFH.\nFor a very small dataframe like this, we can also using View() to open a new tab to review the data, where each column can be sorted from largest to smallest or vice versa. Try viewing it and find the newly created column Prop.WFH. Click on the column name, you should see it is sorted from highest to lowest, and click again, the ranking is reversed.\n\nView(df)\n\n\n\n1.3.1.3 Your first map for Merseyside\nNow let’s try to do our first map in R and allow yourself know more about Merseyside.\nWe will use the library sf and tmap to help us at here. Run the install codes if you haven’t install them. Remember: you will only have to install a package once!!\n\ninstall.packages(\"tmap\",dep =TRUE)\n\nCheck the package version of tmap, as here we need to use tmap over 4.0 version.\n\npackageVersion(\"tmap\") # the version should over 4.0\n\n[1] '4.1'\n\n\nWhen they have been installed, we can start using them\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nYou may find in Week 1 data, we have another file named merseyside_districts.gpkg. A GeoPackage (GPKG) is a file-based format designed for storing geographic data. It supports the efficient storage and exchange of spatial datasets and can be readily used across GIS software such as QGIS and ArcGIS, as well as in programming environments including R and Python.\nWe first read it in by using the st_read() command in library sf.\n\nsf &lt;- st_read(\"merseyside_districts.gpkg\")\n\nReading layer `lad_may_2025_uk_bgc_v2_4306843991635065087__lad_may_2025_uk_bgc_v2' from data source `C:\\Users\\ziye\\Documents\\GitHub\\quant\\labs\\merseyside_districts.gpkg' \n  using driver `GPKG'\nSimple feature collection with 5 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 318351.7 ymin: 377515.4 xmax: 361796.3 ymax: 422866.5\nProjected CRS: OSGB36 / British National Grid\n\n\nThe fastest way to map it is the qtm() function.\n\nqtm(sf)\n\n\n\n\n\n\n\n\nYou can also add the district names on the map - which column in the sf contains district name? Use names(sf) to check for it.\nYes, the column should be LAD25NM. Now let’s ask qtm() to also show the district names.\n\nqtm(sf,text=\"LAD25NM\")\n\n\n\n\n\n\n\n\nBut what if we want to make some meaningful maps, rather than just the boundaries of these five districts of Merseyside?\n\n\n1.3.1.4 Link tabular data to geographical boundaries\nRecall that in our df, we have 14 columns, containing different information about the districts. We can get all their names by using names().\n\nnames(df)\n\n [1] \"LAD21CD\"            \"District\"           \"Population\"        \n [4] \"Households\"         \"Working_population\" \"Students\"          \n [7] \"Unemployed\"         \"Age_over_65\"        \"Disability\"        \n[10] \"No_central_heating\" \"Overcrowding\"       \"Working_from_home\" \n[13] \"Prop.WFH\"          \n\n\nWe can do the same thing for our geographical dataset sf to see what it includes:\n\nnames(sf)\n\n[1] \"LAD25CD\"  \"LAD25NM\"  \"LAD25NMW\" \"BNG_E\"    \"BNG_N\"    \"LONG\"     \"LAT\"     \n[8] \"GlobalID\" \"geom\"    \n\n\nWe can also show the whole sf as\n\nsf\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 318351.7 ymin: 377515.4 xmax: 361796.3 ymax: 422866.5\nProjected CRS: OSGB36 / British National Grid\n    LAD25CD    LAD25NM LAD25NMW  BNG_E  BNG_N      LONG      LAT\n1 E08000011   Knowsley          344762 393778 -2.832979 53.43789\n2 E08000012  Liverpool          339359 390556 -2.913680 53.40833\n3 E08000013 St. Helens          353413 395992 -2.703093 53.45862\n4 E08000014     Sefton          334282 398835 -2.991771 53.48213\n5 E08000015     Wirral          329109 386965 -3.067034 53.37478\n                                GlobalID                           geom\n1 {B4196BFE-EE90-4C31-ABD5-C7E743AE2F9B} MULTIPOLYGON (((341447.1 40...\n2 {4FB47E7A-EF4E-4B9E-BF75-D4FC059CDE61} MULTIPOLYGON (((338860.9 39...\n3 {943F0C6B-EB30-4C00-A42B-F6B3AEC3EFEE} MULTIPOLYGON (((349111.4 40...\n4 {C6FD073B-CBEB-4E78-934A-A8FD11A20F0A} MULTIPOLYGON (((336374.5 42...\n5 {88E9328B-371C-469C-91F1-3479C77D6950} MULTIPOLYGON (((331364.9 39...\n\n\nNow we see that sf includes also the five districts, but also other geographical information. You may notice that although different column names, the first two columns of both df and sf are the district code and district name. This means what potentially we can link this two dataset together - appendix the df to sf to enrich the attributes of our geographical dataset.\n\nmerseyside &lt;- left_join(sf, df,by=c(\"LAD25NM\"=\"District\"))\n\nlet’s check out the new sf2 by View() it:\n\nView(merseyside)\n\nIn the open tab, we see all the df columns are now also attached to the sf, linking by the district names.\n\n\n1.3.1.5 Choropleth map of Merseyside districts\nNow, we can use those new columns we attached from df to sf2 to make some meaningful choropleth maps! Here we make use of the mapping functions in tmap (Remember to run library(tmap) if you haven’t) to do the work for us.\ntmap has a basic syntax (again, do not run this code - its is simply showing the syntax of tmap):\n\n# don't run this or write this into your script!\ntm_shape(data = &lt;data&gt;)+\n  tm_&lt;function&gt;(&lt;variable to be mapped&gt;)\n\nFor example, to map the boundaries of merseyside:\n\ntm_shape(merseyside) + \n  tm_borders()\n\n\n\n\n\n\n\n\nTo add label of district:\n\ntm_shape(merseyside) + \n  tm_borders() + \n  tm_text(\"LAD25NM\")\n\n\n\n\n\n\n\n\nYou might assume the quick mapping function qtm() can achieve the same result, but tmap provides far more flexibility when it comes to aesthetic customization. The easiest way to illustrate tmap is through some examples.\nLet’s start with a simple choropleth map, by using tmap to show the distribution of a continuous variable in different elements of the spatial data (here are the data Merseyside districts are polygons).\nThe code below maps `Students’ as in the Merseyside districts, and shows the district names of each polygon from ‘LAD25NM’ columns. The map below indicates that Liverpool has the highest number of full-time students while Knowsley and St.Helens have the least.\n\ntm_shape(merseyside) + \n  tm_polygons(fill = \"Students\") +   # Variable to map \n  tm_text(\"LAD25NM\")                 # Variable to label\n\n\n\n\n\n\n\n\nBy default tmap picks a shading scheme, the class breaks and places a legend somewhere. All of these can be changed. The code below allocates the tmap plot to map1 (Map 1), change the legend title as “Number of students in Merseyside districts”, and then prints it:\n\nmap1  = tm_shape(merseyside) + \n  tm_polygons(fill=\"Students\",\n              fill.scale = tm_scale(values = \"Greens\"), # change palette to greens\n              fill.legend = tm_legend(title = \"Number of students in Merseyside districts\")\n              ) +# Legend title\n  tm_text(\"LAD25NM\",size=0.8) #size down the label slightly\nmap1\n\n\n\n\n\n\n\n\nAnd of course many other elements included either by running the code snippet defining map1 above with additional lines or by simply adding them as in the code below:\n\nmap1 + \n  tm_scalebar(position = c(\"right\", \"bottom\")) + \n  tm_compass(position = c(\"left\", \"top\"))  # Use \"top\", \"center\", or \"bottom\"\n\n\n\n\n\n\n\n\nWe can also create new variable to the dataset and then map it. The below code chunk first creates a new column, “NoCentralHeating_rate”, by dividing the number of households without access to central heating by the total number of households in each district; it then uses tmap to make a map of the proportion of households without central heating across districts in Merseyside:\n\nmerseyside$NoCentralHeating_rate = merseyside$No_central_heating / merseyside$Households * 100\n\nmap2 = tm_shape(merseyside) + \n  tm_polygons(fill=\"NoCentralHeating_rate\",\n              fill.scale = tm_scale(values = \"Reds\", style = \"jenks\"), #use jenks classification rather than equal\n              fill.legend = tm_legend(title = \"% No Central Heating\")) +\n  tm_text(\"LAD25NM\",size=0.8) +\n  tm_scalebar(position = c(\"right\", \"bottom\")) +  # Add a scale bar at the top-right corner\n  tm_compass(position = c(\"left\", \"top\"))  # Add a compass rose at the top-right corner\nmap2\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 Merseyside neighbourhoods\nNow let’s read in the neighbourhood-level datasets, which include a .csv file of local statistics and the corresponding geographical boundaries.\n\nlsoa_df &lt;- read.csv(\"merseyside_lsoa.csv\")\nlsoa_sf &lt;- st_read(\"LSOA_boundaries.gpkg\")\n\nReading layer `merseyside_LSOA' from data source \n  `C:\\Users\\ziye\\Documents\\GitHub\\quant\\labs\\LSOA_boundaries.gpkg' \n  using driver `GPKG'\nSimple feature collection with 923 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3.200368 ymin: 53.2963 xmax: -2.576743 ymax: 53.6982\nGeodetic CRS:  WGS 84\n\n\nFirst, we take a look at the .csv dataset, which as been read into R as lsoa_df:\n\nView(lsoa_df)\n\nor check the structure of the dataset:\n\nstr(lsoa_df)\n\n'data.frame':   923 obs. of  11 variables:\n $ LSOA21CD          : chr  \"E01006416\" \"E01006418\" \"E01006434\" \"E01006435\" ...\n $ Population        : int  1520 1315 1519 1524 1150 1654 1450 1581 1421 1373 ...\n $ Households        : int  678 567 652 663 490 695 592 622 809 618 ...\n $ Working_population: int  588 547 660 581 546 766 558 570 524 481 ...\n $ Students          : int  59 64 69 82 51 66 65 89 52 72 ...\n $ Unemployed        : num  3.57 2.74 5.11 3.43 1.62 ...\n $ Age_over_65       : num  14.8 17.5 11.7 19.9 26.3 ...\n $ Disability        : num  27.1 30 23.4 29 20.5 ...\n $ No_central_heating: int  16 14 16 7 3 8 9 9 12 9 ...\n $ Overcrowding      : int  24 24 35 28 13 21 29 23 31 22 ...\n $ Working_from_home : int  91 84 102 96 165 171 101 64 66 48 ...\n\n\nSo now, you know how many LSOAs in Merseyside? Yes, there are 923 LSOAs. As we introduced in the Week 1 lecture, LSOA means Super Output Area Lower Area and is commonly used in the Census statistics. Each LSOA represents 1,000 to 3,000 people or 400 to 1,200 households in England and Wales.\n\ndim(lsoa_df)\n\n[1] 923  11\n\n\nUse the quick mapping function qtm() to quickly inspect the geographical boundary dataset lsoa_sf .\n\nqtm(lsoa_sf)\n\n\n\n\n\n\n\n\ncheck how many LSOAs in the boundary dataset - there should also be 923.\n\nnrow(lsoa_sf)\n\n[1] 923\n\n\nTo familiarise yourself with the structures of both datasets, we can use the names() command\n\nnames(lsoa_df)\n\n [1] \"LSOA21CD\"           \"Population\"         \"Households\"        \n [4] \"Working_population\" \"Students\"           \"Unemployed\"        \n [7] \"Age_over_65\"        \"Disability\"         \"No_central_heating\"\n[10] \"Overcrowding\"       \"Working_from_home\" \n\nnames(lsoa_sf)\n\n[1] \"LSOA21CD\" \"LSOA21NM\" \"LAD23CD\"  \"LAD23NM\"  \"geom\"    \n\n\nYou may find that both dataset are recorded at the LSOA level, with LSOA21CD as the key column. As we did with the district-level dataset, we can use left_join() to join these two dataset by their sharing field - LSOA21CD:\n\nlsoa &lt;- left_join(lsoa_sf,lsoa_df,by=\"LSOA21CD\")\n\nNow let’s check the columns of new dataframe lsoa:\n\nnames(lsoa)\n\n [1] \"LSOA21CD\"           \"LSOA21NM\"           \"LAD23CD\"           \n [4] \"LAD23NM\"            \"Population\"         \"Households\"        \n [7] \"Working_population\" \"Students\"           \"Unemployed\"        \n[10] \"Age_over_65\"        \"Disability\"         \"No_central_heating\"\n[13] \"Overcrowding\"       \"Working_from_home\"  \"geom\"              \n\n\nOr open a new tab to view the newly created dataset lsoa by\n\nView(lsoa)\n\nWe can see that some columns contain counts, such as the number of residential population, number of households, number of working population, and number of students. Other columns are expressed as percentages, including unemployment, population aged 65 and over, disability, households without central heating, overcrowded households, and people working from home.\n\n1.3.2.1 Making maps across LSOAs in Merseyside\nUsing the Unemployed column, we can create a map of the unemployment rate across neighbourhoods in Merseyside. Instead of using the default equal-interval breaks, this time we will use a jenks classification with six categories.\n\nmap3 = tm_shape(lsoa) +\n  tm_fill(\n    fill = \"Unemployed\",\n    fill.scale = tm_scale(values = \"GnBu\", \n                          style = \"jenks\", \n                          n = 6), #use jenks classification of 6 categories\n    fill.legend = tm_legend(title = \"% Unemployed\")\n    ) +\n  tm_layout(legend.position = c(\"right\", \"top\"))\nmap3\n\n\n\n\n\n\n\n\nThe above code uses tm_layout(legend.position = c(\"right\", \"top\")) to move the legend inside the map frame, positioning it at the right-top corner.\n\nReplace tm_fill() to tm_polygons() to see how the map changes?\ntm_polygons() is a condense version of tm_fill() + tm_border(). Here if you want show all the LSOA borders, use tm_polygons() instead of tm_fill().\n\n\n\n1.3.2.2 Overlapping tmap objects\ntmap also supports adding or overlaying other data, such as boundaries. Because these are additional spatial data layers, they needs to be added with tm_shape() followed by the usual function.\nRemember we use the code chunk to make the district boundaries of Merseyside? This time let’s change the aesthetic by using grey color as the border color and increase the line width, then we save it also as a tmap object called map_district:\n\nmap_district = tm_shape(merseyside) + \n  tm_borders(col = \"grey50\",lwd=1.5) + #border color as grey, line width as 1.5\n  tm_text(\"LAD25NM\",size = 0.8)\nmap_district\n\n\n\n\n\n\n\n\nTo display both tmap layers together, we can proceed as follows:\n\nmap3 + map_district\n\n\n\n\n\n\n\n\nNow, let’s move to making some more maps - this time showing the proportion of disability across neighbourhoods in Merseyside. Referring back to the columns in lsoa, this time we use the Disability variable. The code below applies a jenks classification and use a different color palette Purples. Also the map frame is removed by frame = FALSE.\n\ntm_shape(lsoa) + \n  tm_fill(fill = \"Disability\",\n          fill.scale = tm_scale(values=\"Purples\", \n                                style = \"jenks\",\n                                n=5),\n          fill.legend = tm_legend(title = \"% Disability\")\n          ) +\n  tm_layout(main.title = \"Merseyside\",#add a main title\n            legend.position = c(\"right\", \"top\"),\n            frame = FALSE)+\n  map_district\n\n\n\n\n\n\n\n\n\n\n1.3.2.3 Create new variables to make maps\nSimilarly, we can make maps from new columns we made ourselves. For example, we can calculate the percentage of student by adding a new column to the dataframe:\n\nlsoa$student.perc = lsoa$Students / lsoa$Population * 100\n\nTo make a map to visualisation the spatial distribution of student percentage. The code below uses n=6 to increase the classification categories to 6 rather than default 5.\n\ntm_shape(lsoa) + \n  tm_fill(fill = \"student.perc\",\n          fill.scale = tm_scale(values=\"Blues\",\n                                style = \"jenks\",\n                                n=6),\n          fill.legend = tm_legend(title = \"% of Student\",\n                                  title.size = 0.8) #legend title change smaller font\n          ) +\n  tm_layout(main.title = \"Merseyside\",\n            frame = FALSE,\n            legend.position = c(\"right\", \"top\"))+\n  map_district\n\n\n\n\n\n\n\n\nTo change the palette, RColorBrewer provides different palette choices:\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n\n\n\n1.3.2.4 In a nutshell\nIf we want to make a map to show the rate of no central heating households in all the neighbourhoods in Merseyside, we need to create a new variable no.central.heating.perc as the result of dividing households without central heating by total households in each LSOA. The code below combines all the cartographic elements together:\n\nlsoa$no.central.heating.perc = lsoa$No_central_heating / lsoa$Households * 100\n\ntm_shape(lsoa) + \n  tm_fill(fill = \"no.central.heating.perc\",\n          fill.scale = tm_scale(values=\"YlOrRd\",\n                                style = \"jenks\",n=6),\n          fill.legend = tm_legend(title = \"% of no central heating households\", \n                                  title.size = 0.8)\n        ) +\n  tm_layout(main.title = \"Merseyside\",\n            main.title.size=1.2,\n            frame = FALSE) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scalebar(position = c(\"right\", \"bottom\")) +\n  tm_shape(merseyside) +               # Add another spatial layer (Merseyside boundary)\n  tm_borders(col = \"black\", lwd = 1) +  # Draw the boundaries with black lines of width 1\n  tm_text(\"LAD25NM\",col = \"blue\",size = 0.8)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html#summary",
    "href": "labs/01.GettingStartedinRStudio.html#summary",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nThe aim of this session has been to familiarise you with the R environment if you have not used R before. If you have but not for a while, then hopefully this has acted as a refresher. Some key things to take away are:\n\nR is a learning curve, and like driving the more your practice the better you become.\nYour job is to try to understand what the code is doing and not to remember the code.\nTo help with this, you should add your own comments to the script to help you understand what is going on when you return to them. Comments are prefaced by a hash (#) that is ignored by R.\nAlways set your working directory to the sub-folder containing your R script.\nAlways run your code from an R script… always!\n\n\n1.4.1 References\nBrunsdon, Chris, and Lex Comber. 2018. An Introduction to r for Spatial Analysis and Mapping (2e). Sage.\nComber, Lex, and Chris Brunsdon. 2021. Geographical Data Science and Spatial Data Analysis: An Introduction in r. Sage.\nHarris, Richard. 2016. Quantitative Geography: The Basics. Sage.\nOther good on-line get started in R guides include:\n\nThe Owen guide (only up to page 28) : https://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf\nAn Introduction to R - https://cran.r-project.org/doc/contrib/Lam-IntroductionToR_LHL.pdf\nR for beginners https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/01.GettingStartedinRStudio.html#formative-tasks",
    "href": "labs/01.GettingStartedinRStudio.html#formative-tasks",
    "title": "1  Lab: Getting Started in RStudio - Knowing Merseyside",
    "section": "1.5 Formative Tasks",
    "text": "1.5 Formative Tasks\nTask 1 From the district level dataset “merseyside.csv”, extract household information for the Liverool and Wirral districts. The variables to be included are “Households”, “No_central_heating” and “Overcrowding”.\n\ndf &lt;- read.csv(\"merseyside.csv\")  \ndf[c(2,5),c(\"District\",\"Households\",\"No_central_heating\",\"Overcrowding\")]\n\n   District Households No_central_heating Overcrowding\n2 Liverpool     207491               4822         7352\n5    Wirral     143253               2125         2355\n\n\nTask 2 Use the dataset “merseyside_lsoa.csv”, plot the Disability against Age_over_65 from the data frame.\n\nlsoa_df &lt;- read.csv(\"merseyside_lsoa.csv\") \nplot(Disability~Age_over_65, data = lsoa_df) \n\n\n\n\n\n\n\n# or \nplot(lsoa_df$Disability, lsoa_df$Age_over_65)\n\n\n\n\n\n\n\n\nTask 3 Use the district level dataset, how many households in total in Merseyside?\n\ndf &lt;- read.csv(\"merseyside.csv\")\nsum(df$Households)\n\n[1] 620903\n\n\nTask 4 Use the district level dataset, what is the overall proportion of the ageing population (age over 65) in Merseyside?\n\nsum(df$Age_over_65)/sum(df$Population)\n\n[1] 0.1920626\n\n\nTask 5 Use the LSOA level dataset, what is the average proportion of the ageing population (age over 65) across all the neighbourhoods of Merseyside?\n\ndf$ageing_rate = df$Age_over_65 / df$Population * 100\nmean(df$ageing_rate)\n\n[1] 19.59824\n\n\nTask 6 Create a map showing the spatial distribution of the proportion of ageing population (age over 65) over LSOAs in Merseyside? (use Jenks classification of 7 categories).\n\nlsoa$ageing_rate = lsoa$Age_over_65 / lsoa$Population * 100\n\ntm_shape(lsoa) + \n  tm_fill(fill = \"ageing_rate\",\n          fill.scale = tm_scale(values=\"PuRd\",style = \"jenks\",n=7),\n          fill.legend = tm_legend(title = \"% Age over 65\", title.size = 0.8)\n        ) +\n  tm_layout(main.title = \"Merseyside\",\n            main.title.size=1.2,\n            frame = FALSE) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scalebar(position = c(\"right\", \"bottom\")) +\n  tm_shape(merseyside) +               # Add another spatial layer (Merseyside boundary)\n  tm_borders(col = \"black\", lwd = 1) +  # Draw the boundaries with black lines of width 1\n  tm_text(\"LAD25NM\",col = \"blue\",size = 0.8)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Getting Started in RStudio - Knowing Merseyside</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html",
    "href": "labs/02.ExploratoryDataAnalysis.html",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "",
    "text": "2.1 Overview\nThis week’s practical session will draw upon the UK 2024 constituency election dataset. We will revisit some libraries and functions we have used last week, but also learn to do our first exploratory data analysis by foundmental R coding. We will do:\nYou may wish to recap this week’s lecture: Lecture 02.pptx",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#overview",
    "href": "labs/02.ExploratoryDataAnalysis.html#overview",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "",
    "text": "Loading and examining tabular data (like spreadsheets) and geographical data into R (same as Week 1)\nExploratory Data Analysis (or EDA) of numeric variables and categorical variables\nUsing histogram, boxplot, barplot to understand the distribution of variables\nVariable interactions, particularly cross-tabulation and between-group comparison",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#clear-the-decks",
    "href": "labs/02.ExploratoryDataAnalysis.html#clear-the-decks",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "2.2 Clear the decks",
    "text": "2.2 Clear the decks\n\nFor this Week 2 session, create a sub-folder called Week2 in your ENVS162 folder on your M-Drive.\nOpen RStudio\nOpen a new R Script for your Week 2 work, rename it as Week2.R and save it in your newly created Week 2 folder, under M drive -&gt; ENVS162 folder. This is exactly the step we did in Week 1, and we will do this every week to Week 5.\nCheck whether there is any previous left dataframes in your RStudio in the upper-right side Environment pane. You can always use the  to clear all the dataframes in your environment and make it all clean. For the same aim, you can run the below code:\n\n\nrm(list = ls())\n\nThis command will clear RStudio’s memory, removing any data objects that you have previously created.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#open-libraries",
    "href": "labs/02.ExploratoryDataAnalysis.html#open-libraries",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "2.3 Open libraries",
    "text": "2.3 Open libraries\nIn Week 1 we have installed essential R package tidyverse, sf, and tmap. Remember if any package has been installed, then we don’t need to re-install them. Instead, we use library() command to import and use them.\nAs ever, when you start a new session in RStudio, you need to load the packages you wish to use into memory. Similarly, there are tidyverse, tmap and sf packages we’ve used last week.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nIf R returns Error: there is no package called ‘***’. Then it means that the package ‘***’ has not been installed in the PC you current use. Therefore you need to install them first. Switch back to Week 1 instruction - Getting set up with RStudio - Your first R code - Package part to refresh yourself how to do this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#parliamentary-constituency-data",
    "href": "labs/02.ExploratoryDataAnalysis.html#parliamentary-constituency-data",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "2.4 Parliamentary Constituency Data",
    "text": "2.4 Parliamentary Constituency Data\n\n2.4.1 Load the dataset\nIn 2024 the UK held a general election. Download the file uk_constituencies_2024.csv from our Canvas module page Week 2. Save the dataset in your M drive - ENVS162 - Week 2 folder, alongside the Week 2. R script. Read in the dataset exactly in the same way as we did in Week 1 - insert the below code line and run it:\n\npc_data &lt;- read.csv(\"uk_constituencies_2024.csv\",stringsAsFactors = TRUE)\n\nThe datasheet captures a range of information relating to this election, and to the nature of each parliamentary constituency. By using read.csv() command, we use R to store the dataset in a dataframe called pc_data (short for Parliamentary Constituency data).\nThe pc_data dataset should appear in your RStudio Environment Pane on the upper right part, indicating that it has now been loaded into memory, and is available for analysis.\nFor your information only, the pc_data dataset has been assembled by combining information from the following sources.\n\nHoC-GE2024-results-by-constituency.xlsx\nSource: Cracknell et al (2024) General election 2024 results, Research Briefing, House of Commons Library. https://commonslibrary.parliament.uk/research-briefings/cbp-10009/\nDemographic-data-for-new-parliamentary-constituencies-May-2024.xlsx\nSource: House of Commons Demographic data for Constituencies https://commonslibrary.parliament.uk/data-for-new-parliamentary-constituencies/\nNatCen Constituency Data_20 June 2024.xlsx\nSource: National Centre for Social Research (2024) Parliamentary constituency look-up, https://natcen.ac.uk/constituency-look-up (date accessed: 07-02-2025)\nnomis_2025_01_14_101749.xlsx\nSource: Income data from Annual Survey of Hours and Earnings (ASHE), collecgted by the Office for National Statistics. https://www.nomisweb.co.uk/datasets/asher\n\n\n\n2.4.2 Familiar with the dataset and variable types\nIn the pc_data dataset each row represents a different UK Parliamentary Constituency. Use the View() command to familiarise yourself with the variables contained in the dataset.\n\nView(pc_data)\n\nUse the nrow( ) or dim() command to find out how many Parliamentary Constituencies (and therefore MPs) there are in the UK.\n\nnrow(pc_data)\n\ndim(pc_data)\n\nWhenever we try to understand the variables in one dataset, the first question to ask ourselves is: “are they continuous or categorical ?”\nContinuous variables are numeric measures of some quantity, such as a count or percentage or a precise value. E.g. number of valid votes; % of persons unemployed etc.\nIn contrast, categorical variables simply group observations into categories or ranges. E.g. name of the winning party; age group etc.\nExplore the structure of the data table using the str function and have examined it by head functions.\n\nstr(pc_data)\n\nhead(pc_data)\n\nWe can see that we have numeric data in integers (int) form (these are counts or whole numbers) and continuous (num) form, and the character variables (text) have been converted to factors. For each of these data types we can generate numeric and visual summaries and we can also see how they interact with each other.\nThe pc_data dataset contains three basic sets of information about each Parliamentary Constituency:\n\nconstituency identifiers - gss_code and pc_name\npopulation information for each constituency, ranging from the total population and number of households through to the % in various categories to information about local house prices, salaries and crime rates\n2024 election results ranging from the winning MP and party through to the size of the electorate and vote turnout, and the share of votes received by each party\n\n\n\n2.4.3 Exploratory Data Analysis (EDA)\n\n2.4.3.1 Numeric variables\nYou can use the pc_data dataset to extract some headline results from the 2024 General Election. Starting with the simplest case, the distribution of a single numeric variable whether continuous or count based can be examined numerically using the summary function.\n\n#valid votes in constituency\nsummary(pc_data$valid_votes)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13528   40397   44628   44322   48607   57744 \n\n\n\n# percentage of White British in constituency\nsummary(pc_data$pct_White_British)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  8.937  71.534  86.208  78.006  92.843  98.664 \n\n\nA visual approach is more intuitive. The code below plots histograms of the two variables, using the hist function. The logic under-pinning a histogram is that a continuous variable (in this case valid_votes and pct_White_British) is temporarily regrouped into categories, using an equal-interval approach. The number of observations (in this case, constituencies) that fall into each equal-interval category then determine the height of each column in the histogram.The comments after each line shall inform you what the main and xlab in the function mean:\n\n# histograms\nhist(pc_data$valid_votes, \n     main = \"Valid votes in constituency\", #change chart title\n     xlab = \"Votes\")     #change x axis label              \n\n\n\n\n\n\n\n\n\nhist(pc_data$pct_White_British, \n     main = \"Histogram of % White British in constituency\", \n     xlab = \"Percentage\", \n     col = \"dodgerblue\")   #change bar color to dodgerblue\n\n\n\n\n\n\n\n\nYou may noticed that valid_votes has a relatively normal, bell-shaped distribution whereas the pct_White_British variable is left skewed (negatively) distribution.\nWe can examine the how these distributions relate to central tendencies (mean, median) and spread, using standard deviations for means and the IQR (Inter-Quartile Range) for medians.\nFrom the numeric summary above, the mean of pc_data$valid_votes is 44,322. We can determine the spread around this value by calculating the standard deviation for our sample , as is returned by the sd function in R:\n\nsd(pc_data$valid_votes)\n\n[1] 5697.665\n\n\nFor a normal distribution, about 68% of observations lie within 1 standard deviation of the mean, and about 95% lie within 2 standard deviations of the mean. So this suggest that 68% of the valid votes are within 5,698 votes of the mean of 44,322 votes, i.e. 38,624 (44322-5698) and 50020 (44322+5698).\nWe can augment the histogram of the valid_votes variable with this information, requesting R to increase the intervals to 50, and using the abline function to add lines to create the figure below:\n\n# histogram\nhist(pc_data$valid_votes, col=\"forestgreen\", main=\"Valid votes (with mean and std dev)\", \n     breaks = 50, xlab=\"Votes\")\n# calculate and add the mean\nmean_val = mean(pc_data$valid_votes)\nabline(v = mean_val, col = \"orange\", lwd = 3)\n# calculate and add the standard deviation lines around the mean\nsdev = sd(pc_data$valid_votes)\n# minus 1 sd\nabline(v = mean_val-sdev, col = \"red\", lwd = 3, lty = 2)\n# plus 1 sd\nabline(v = mean_val+sdev, col = \"red\", lwd = 3, lty = 2)\n\n\n\n\n\n\n\n\nThis histogram show the variable valid_votes with the orange solid line as the mean, and dashed red line as the standard deviation. Note that in the call to abline above, note the specification of different line types (lty) and line widths (lwd). Later on, you could explore these as described here.\nWe can also use density curve to present the variable distribution:\n\nplot(density(pc_data$valid_votes), \n     main = \"Valid votes\")\n\n\n\n\n\n\n\n\n\nplot(density(pc_data$pct_White_British), \n     main = \"% of White British in Constituency\", \n     col=\"salmon\", \n     lwd=2)\n\n\n\n\n\n\n\n\nBoxplots show the same information but here we can see a bit more of the nature of the distribution.\nRecap Week 2 lecture, the dark line shows the median, the box represents the interquartile range (from the 1st to the 3rd quartile), the whiskers extend to the most extreme non-outlying values, and the dots indicate outliers.\n\nNow let’s check out the boxplots of these two variables:\n\nboxplot(pc_data$valid_votes,\n        horizontal=TRUE, \n        main = \"Valid votes\", \n        xlab='Votes', \n        col = \"gold\")\n\n\n\n\n\n\n\n\n\nboxplot(pc_data$pct_White_British,\n        horizontal=TRUE, \n        main = \"% of White British\", \n        xlab='Percentage', \n        col = \"hotpink\")\n\n\n\n\n\n\n\n\n\nIs the median line in the centre of the box? If the median line is not in the middle of the box, it means the data are skewed, with greater spread on one side of the median.\n\nMedian closer to the bottom of the box -&gt; right-skewed (positively skewed) -&gt; some big values pulling up\nMedian closer to the top of the box -&gt; left-skewed (negatively skewed) -&gt; some small values dragging down\n\nAre the whiskers the same length? This indicates skewness in the distribution. The data have a longer tail on the side of the longer whisker, meaning values are more spread out in that direction.\nAre there any outliers? Who has more? Outliers clustered on one side indicate that extreme observations occur predominantly in one tail of the distribution. More outliers indicates the variable hs more extreme or unusual values, possibly a heavier-tailed distribution. But this doesn’t mean the variable is bad or more variable overall.\nThe size of the boxes? The box represents the interquartile range (IQR), which is the middle 50% of the data as we call them typical. A larger box indicates greater variability in the middle 50% of the variable; a smaller box suggests that values are more tightly clustered around the median.\n\nNow, you should be able to compare how the two kinds of distribution are shown in the the boxplots with a trained eye: The pct_White_British variable is left-skewed, indicating that more values are spread towards lower percentages of White British in the constituency. The box is also larger than that of valid_votes, which suggests greater variability in the central 50% of the data. In addition, the longer lower whisker and the presence of more outliers on the lower end further reinforce the left-skewed distribution.\n\nIn summary, numeric variable distributions, of counts and continuous data, should be investigated as an initial step in any data analysis. There are a number of metrics and graphical functions (tools) for doing this including summary(), hist() plot(density()) and boxplot().\n\n\n\n2.4.3.2 Categorical variables\nSome of the character variables could be considered as categorical, representing a grouping or classification of some kind, as described above. In these cases we are interested in the count or frequency of each class in the classification, which we can examine numerically or graphically.\nThe simplest way to examine classes is to put them into a table of counts. The table function is very useful and in the code below it is applied to one of the categorical variables in the survey data:\nSo firstly we use the table() command to find the number of MPs elected to each Party. [Hint: use the first_party variable].\n\ntable(pc_data$first_party)\n\n\n       Alliance    Conservative             DUP           Green     Independent \n              1             121               5               4               6 \n         Labour        Lib Dems     Plaid Cymru          Reform            SDLP \n            411              72               4               5               2 \n      Sinn Fein             SNP         Speaker Ulster Unionist  Unionist Voice \n              7               9               1               1               1 \n\n\nThese can be made a bit more tabular in format with the data.frame function, which takes the table operation as its input:\n\ndata.frame(table(pc_data$first_party))\n\n              Var1 Freq\n1         Alliance    1\n2     Conservative  121\n3              DUP    5\n4            Green    4\n5      Independent    6\n6           Labour  411\n7         Lib Dems   72\n8      Plaid Cymru    4\n9           Reform    5\n10            SDLP    2\n11       Sinn Fein    7\n12             SNP    9\n13         Speaker    1\n14 Ulster Unionist    1\n15  Unionist Voice    1\n\n\nHowever, if we not only care about the count of MPs but also the proportion? Then we can make a good use of our tidyverse library to run the following code line.\n\npc_data %&gt;%\n  count(first_party) %&gt;%\n  mutate(pct = round(n / sum(n) * 100,1))\n\n       first_party   n  pct\n1         Alliance   1  0.2\n2     Conservative 121 18.6\n3              DUP   5  0.8\n4            Green   4  0.6\n5      Independent   6  0.9\n6           Labour 411 63.2\n7         Lib Dems  72 11.1\n8      Plaid Cymru   4  0.6\n9           Reform   5  0.8\n10            SDLP   2  0.3\n11       Sinn Fein   7  1.1\n12             SNP   9  1.4\n13         Speaker   1  0.2\n14 Ulster Unionist   1  0.2\n15  Unionist Voice   1  0.2\n\n\nHere you are using two very useful functions in the library tidyverse.\nFirst, the count() calculate the frequency of different categories in the first_party and use a new column n to store the frequencies - it actually do the same thing as above code, but better in presenting as a table;\nSecond, the mutate() function to assist use create a new column pctand fill in the value by the calculation pct = n / sum(n) * 100.\nThe %&gt;% is used to link these two commands: count() and mutate(). You can insert %&gt;% in your R script by using ctrl + shift + M for Windows and Cmd + Shift + M on Mac.\nTherefore, when you run the code, you will see a table showing as three column: first part name, a new column automatically named as n by R after the count() function, and count of the party MPs, and a new column called pct and with values calculated by n / sum(n) * 100. We use round() function to keep only 1 digits for the pct.\nWe can also improve the code to make a better table presentation. You may find the comment text after each code line would be useful to understand what R has done to the pc_data.\n\n#Calculate the frequency and percentage of different categories for \"first_party\"\npc_data %&gt;%\n  count(first_party) %&gt;%\n  mutate(pct = round(n / sum(n) * 100,1)) %&gt;% \n  arrange(desc(n)) %&gt;%   #sort the table by number of MPs from more to less\n  setNames(c(\"First Party\", \"Number of MPs\", \"% of MPs\"))  #rename table column names\n\n       First Party Number of MPs % of MPs\n1           Labour           411     63.2\n2     Conservative           121     18.6\n3         Lib Dems            72     11.1\n4              SNP             9      1.4\n5        Sinn Fein             7      1.1\n6      Independent             6      0.9\n7              DUP             5      0.8\n8           Reform             5      0.8\n9            Green             4      0.6\n10     Plaid Cymru             4      0.6\n11            SDLP             2      0.3\n12        Alliance             1      0.2\n13         Speaker             1      0.2\n14 Ulster Unionist             1      0.2\n15  Unionist Voice             1      0.2\n\n\nIn this code chuck, we requested four command to the dataframe pc_data, and linked them with %&gt;% :\n\ncount() function to summarises the data by counting the number of observations in each group;\nmutate() function to create a new column named “pct” as we did above;\narrange() function sorts the rows, desc(n) function decending the norder of n, together they order the category with the highest counts first;\nsetNames() function renames the results.\n\nSimilarly, we can use the categorical variable crime_rate in the pc_data dataset to understand the crime status of all the constituencies:\n\n#Calculate the frequency and percentage of different categories for \"crime_rate\"\npc_data %&gt;%\n  count(crime_rate) %&gt;% \n  mutate(pct = round(n / sum(n)*100, 1))  %&gt;% \n  arrange(desc(n)) %&gt;%\n  setNames(c(\"Crime rate\", \"Number of Constituency\", \"% of Constituency\"))\n\n   Crime rate Number of Constituency % of Constituency\n1 Much higher                    118              18.2\n2      Higher                    115              17.7\n3     Average                    114              17.5\n4       Lower                    114              17.5\n5  Much lower                    114              17.5\n6        &lt;NA&gt;                     75              11.5\n\n\nWhat you have learnt from the result table? It seems that there is a quite equally distribution across the five categories of crime rate, although there are 11.5% of the constituency are missing their crime rates.\nCategorical data can be visualised using bar plots of the tabularised data. The code below does this by creating a table, changing the names of the table and passing that to the barplot function:\n\n#calculate the frequency of first party and save the result in table 'tab', present tab as a barplot\ntab = table(pc_data$first_party)\nbarplot(tab)\n\n\n\n\n\n\n\n\nIt is very simple to get the barplot from the result of table(). But it may need some improvement. As you may noticed, there are many bars have rarely no values, and too crowd x axis labels makes some bar label can’t able to show. Therefore, we probably don’t need to show all the parties, but only the top 8 in terms of their winning constituencies.\n\n#sorted tab by the frequency from highest to lowest, present the top 8 of the sorted tab \ntab_sorted &lt;- sort(tab,decreasing = TRUE)\nbarplot(head(tab_sorted,8),las = 2, main = \"Top 8 of first party\") \n\n\n\n\n\n\n\n\nIn this code chunk, we first use sort() to reorder the tab ranking the parties based on the counts from highest to lowest. You may ask R to show how tab_sorted looks like:\n\n#show tab_sorted\ntab_sorted\n\n\n         Labour    Conservative        Lib Dems             SNP       Sinn Fein \n            411             121              72               9               7 \n    Independent             DUP          Reform           Green     Plaid Cymru \n              6               5               5               4               4 \n           SDLP        Alliance         Speaker Ulster Unionist  Unionist Voice \n              2               1               1               1               1 \n\n\nThen, using barplot(), we plot the top 8 rows of tab_sorted, with las = 2 used to rotate the axis labels so they are displayed vertically and remain readable.\n\n\n2.4.3.3 Categorical to categorical: cross-tabulation\nIn EDA, it is also important to understand the relationship between variables. Now it is the time to do the variable interactions. Let’s first start with interact one categorical variable to another categorical variable. In many situations, it can be called cross-tabulation.\nThe relationship between two sets of classes or categories can be explored using correspondence tables created by the table function. Here we can cross tabulate the two categorical variables that we have already familiar with: first_party and crime_rate:\nIf we want to examine how the distribution of crime-rate categories varies for each first party. The question can be: for each political party, how are its wins distributed across different crime-rate levels?\n\n# cross-tabulation first_party vs.crime_rate\n\ntable(pc_data$first_party, pc_data$crime_rate)\n\n                 \n                  Average Higher Lower Much higher Much lower\n  Alliance              0      0     0           0          0\n  Conservative         25      6    44           0         41\n  DUP                   0      0     0           0          0\n  Green                 1      0     0           1          2\n  Independent           0      3     0           2          0\n  Labour               79    101    47         115         32\n  Lib Dems              6      2    20           0         38\n  Plaid Cymru           0      0     3           0          1\n  Reform                2      3     0           0          0\n  SDLP                  0      0     0           0          0\n  Sinn Fein             0      0     0           0          0\n  SNP                   0      0     0           0          0\n  Speaker               1      0     0           0          0\n  Ulster Unionist       0      0     0           0          0\n  Unionist Voice        0      0     0           0          0\n\n\nConversely, we can also examine how the distribution of first parties varies across different crime-rate categories.The question this time is: for each crime-rate category, how are different parties distributed?\n\n# cross-tabulation crime_rate vs first_party\n\ntable(pc_data$crime_rate, pc_data$first_party)\n\n             \n              Alliance Conservative DUP Green Independent Labour Lib Dems\n  Average            0           25   0     1           0     79        6\n  Higher             0            6   0     0           3    101        2\n  Lower              0           44   0     0           0     47       20\n  Much higher        0            0   0     1           2    115        0\n  Much lower         0           41   0     2           0     32       38\n             \n              Plaid Cymru Reform SDLP Sinn Fein SNP Speaker Ulster Unionist\n  Average               0      2    0         0   0       1               0\n  Higher                0      3    0         0   0       0               0\n  Lower                 3      0    0         0   0       0               0\n  Much higher           0      0    0         0   0       0               0\n  Much lower            1      0    0         0   0       0               0\n             \n              Unionist Voice\n  Average                  0\n  Higher                   0\n  Lower                    0\n  Much higher              0\n  Much lower               0\n\n\nWhat insights now you can learn from these cross-tabulation? We will examine methods for determining whether the cross-tabulated counts and their differences are significant (i.e. would not be expected by chance) in later weeks.\n\n\n2.4.3.4 Continuous to categorical: compare between groups\nWe may also be interested in the exploring differences and similarities in the continuous variables associated with for each categorical class. This can be done using multiple boxplots.\nIf we want to make boxplots of each party by the value median house price of the constituencies who vote for this party as their first party:\n\n#create boxplots by median_house_price for each first_party\n\npar(mar = c(10, 4, 4, 2))  # increase the margin of the chart for each side: bottom, left, top, right\n\nboxplot(median_house_price ~ first_party, \n        data = pc_data, \n        las = 2, #vertical present item label\n        xlab=\"\", #no x axis label\n        ylab=\"\", #no y axis label,\n        main=\"Compare constituency median house price vs. first party\")\n\n\n\n\n\n\n\n\nIf we want to explore the % of White British in the constituencies and compare the distribution between different MP genders. The code below shows that, compared with constituencies represented by male MPs, those with female MPs generally have a lower proportion of White British population. This distribution also exhibits clear skewness towards lower percentages of White British residents.\n\nboxplot(pct_White_British ~ mp_gender, \n        data = pc_data,col=c(\"red\", \"yellow3\"), #specify bar colors\n        horizontal = TRUE) #horizonal the boxplot\n\n\n\n\n\n\n\n\nWe can do this numerically as well, but it is a bit more convoluted using the with and aggregate functions:\n\nwith(pc_data, aggregate(pct_White_British, by=list(mp_gender) , FUN=summary))\n\n  Group.1    x.Min. x.1st Qu.  x.Median    x.Mean x.3rd Qu.    x.Max.\n1  Female 15.587153 65.775762 84.276710 75.207471 92.374361 98.663686\n2    Male  8.936846 74.343213 87.344901 79.907108 93.326348 98.652616",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#make-your-own-map-for-the-election-result",
    "href": "labs/02.ExploratoryDataAnalysis.html#make-your-own-map-for-the-election-result",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "2.5 Make your own map for the election result",
    "text": "2.5 Make your own map for the election result\nHaving established how many MPs and votes each party got, it is time to look at the geography of the election outcome. To do this we need to link a set of digital boundaries for Parliamentary Constituencies with our pc_data dataset.\n\n2.5.1 Read in Parliamentary Constituency Boundaries\nDigital boundaries for the Parliamentary Constituencies used in the 2024 General Election can be found in the files uk_constituencies_2024.gpkg from the Cavans module page. Download and save it in the Week 2 folder as well.\nAs Week 1, we use st_read() function from library(sf) to read in this geographical boundary dataset.\n\n#read the boundaries as a spatial dataset\n\npc_map &lt;- st_read(\"uk_constituencies_2024.gpkg\")  \n\nReading layer `uk_constituencies_2024' from data source \n  `C:\\Users\\ziye\\Documents\\GitHub\\quant\\labs\\uk_constituencies_2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 650 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 191.9359 ymin: 7423.9 xmax: 655599.6 ymax: 1218591\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\n2.5.2 Inspect the spatial dataset\nUse the names() or str() to know the contents, or as above use View() to open and check. Check by yourself the number of rows and columns of the map data:\n\nstr(pc_map)\n\n\nView(pc_map)\n\nThe pc_map dataset contains the standard set of Parliamentary Constituency boundaries:\n\n#make a map of constituency \ntm_shape(pc_map) + #map a spatial data\n  tm_polygons() #map it as polygons\n\nor we can make a colorful map by using different color for different regions:\n\n#make a map of constituency and color each polygon based on \"region_name\", using different color from palette \"Set3\" for each \"region_name\"\n\ntm_shape(pc_map) + #map a spatial data\n  tm_polygons(\"region_name\",\n              palette=\"Set3\")  \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Set3\" is named\n\"brewer.set3\"\nMultiple palettes called \"set3\" found: \"brewer.set3\", \"hcl.set3\". The first one, \"brewer.set3\", is returned.\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Link boundaries to pc_data\nIn order to map the election results contained in the pc_data dataset, we need to join it to a set of digital boundaries using the left_join( ) command - you should have already familiar with this from Week 1.\nIn your inspection of the pc_data and pc_map datasets, you may have noticed that they all have two variables in common. The first is a unique identifier for each Parliamentary Constituency: gss_code. The second is the name of the constituency: pc_name.\nWe can use these two variables to first link the pc_data dataset to the standard map:\n\n#left join pc_data to pc_map, joining when gss_code from pc_map equals to pc_name in pc_data\n\npc_map_new &lt;- left_join(pc_map, pc_data, by = c(\"gss_code\", \"pc_name\")) \n\nAs ever, having created a new dataset, use the dim( ), str( ), names( ) and View( ) commands to check its contents are as you would expect.\n\n\n2.5.4 Map the election result\nHaving joined the pc_data dataset with a set of digital boundaries, it becomes a simple matter to map the election results using the mapping skills covered in Week 1:\n\n#make a map for the pc_map_new, fill the colors for each polygon based on \"first_party\", using colors from palette \"Paired\"\n\ntm_shape(pc_map_new) + #map a spatial data\n  tm_polygons(fill = \"first_party\", \n              fill.scale = tm_scale(values=\"Paired\"))  #map it as polygons, use different colors by first_party\n\n\n\n\n\n\n\n\nThis time, let’s change the mode of tmap by using tmap_mode() from default “plot” to “view” for an interactive map:\n\n# make the map interactive\ntmap_mode(\"view\")\ntm_shape(pc_map_new) + \n  tm_polygons(\"first_party\")  \n\nThis time, in your right-bottom pane, the map should be plotted in Viewer tab as an interactive map. You can zoom in/out to explore your map, for a better view, you can click the  to open a webpage.\nYou may need to switch tmap_mode() back to “plot” for a static map making:\n\n#switch back to plot static mode for later use\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\n\nNow what pattern you can observe from the interactive map you just made?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/02.ExploratoryDataAnalysis.html#formative-tasks",
    "href": "labs/02.ExploratoryDataAnalysis.html#formative-tasks",
    "title": "2  Lab: Exploratory Data Analysis - UK Election",
    "section": "2.6 Formative tasks",
    "text": "2.6 Formative tasks\nTask 1 Write code to get how many columns and rows in the UK constituency boundary dataset pc_map?\n\npc_map &lt;- st_read(\"uk_constituencies_2024.gpkg\")\n\nReading layer `uk_constituencies_2024' from data source \n  `C:\\Users\\ziye\\Documents\\GitHub\\quant\\labs\\uk_constituencies_2024.gpkg' \n  using driver `GPKG'\nSimple feature collection with 650 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 191.9359 ymin: 7423.9 xmax: 655599.6 ymax: 1218591\nProjected CRS: OSGB36 / British National Grid\n\nncol(pc_map)\n\n[1] 10\n\nnrow(pc_map)\n\n[1] 650\n\n#both col and row\ndim(pc_map)\n\n[1] 650  10\n\n\nTask 2 Using the UK constituency boundary dataset pc_map, write code to get descriptive summary the area (variable: sq_km) of all the constituencies.\n\nsummary(pc_map$sq_km)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    6.80    33.65   106.45   375.01   351.98 11634.40 \n\n\nTask 3 Write some codes to plot a histogram of the pct_invalid_votes variable in the constituency election dataset, with lines showing the mean and the standard deviation around the mean. Try add breaks into the function and change breaks from 10, 20 to 50 and see how the histogram changed.\n\npc_data &lt;- read.csv(\"uk_constituencies_2024.csv\",stringsAsFactors = TRUE)\n\n# histogram\nhist(pc_data$pct_invalid_votes, col = \"thistle\", main = \"Invalid votes in constituency\", xlab = \"Percentage\", breaks = 50)\n# calculate and add the mean\nmean_val = mean(pc_data$pct_invalid_votes, na.rm = T)\nabline(v = mean_val, col = \"purple\", lwd = 3)\n# calculate and add the standard deviation lines around the mean\nsdev = sd(pc_data$pct_invalid_votes, na.rm = T)\nabline(v = mean_val-sdev, col = \"tan\", lwd = 3, lty = 2)\nabline(v = mean_val+sdev, col = \"tan\", lwd = 3, lty = 2)\n\n\n\n\n\n\n\n\nTask 4 Write codes to create boxplots for pct_in_migration, pct_UK_born and pct_owned (owning upright household):\n\nboxplot(pc_data$pct_in_migration,horizontal=TRUE, main = \"In migration\", xlab='Percentage', col = \"gold\")\n\n\n\n\n\n\n\nboxplot(pc_data$pct_UK_born ,horizontal=TRUE, main = \"UK born\", xlab='Percentage', col = \"pink\")\n\n\n\n\n\n\n\nboxplot(pc_data$pct_owned,horizontal=TRUE, main = \"Owning house upright\", xlab='Percentage', col = \"lightblue\")\n\n\n\n\n\n\n\n#to compare them together\nboxplot(pc_data[,c(\"pct_in_migration\",\n                   \"pct_UK_born\",\n                   \"pct_owned\"\n                   )],\n        horizontal=TRUE, \n        main = \"Owning house upright\", \n        xlab='Percentage', \n        col = c(\"gold\",\"pink\",\"lightblue\")\n        )\n\n\n\n\n\n\n\n\nTask 5 Write codes to summary the counts of MPs in each gender (this is in the mp_gendervariable), presenting the table with percentage and showing the barplot.\n\npc_data %&gt;% \n  count(mp_gender) %&gt;% \n  mutate(pct = round(n/sum(n)*100,1))\n\n  mp_gender   n  pct\n1    Female 263 40.5\n2      Male 387 59.5\n\ntab = table(pc_data$mp_gender)\n\nbarplot(tab,main = \"MP gender in constituency\")\n\n\n\n\n\n\n\n\nTask 6 Cross tabulation region_name to mp_gender by using the newly created pc_map_new dataset, which joined by the constituency boundary and election dataset.\n\ntable(pc_map_new$region_name, pc_map_new$mp_gender)\n\n                          \n                           Female Male\n  East Midlands                20   27\n  East of England              16   45\n  Greater London               38   37\n  North East                   12   15\n  North West                   31   42\n  Northern Ireland              5   13\n  Scotland                     20   37\n  South East                   39   52\n  South West                   19   39\n  Wales                        15   17\n  West Midlands                25   32\n  Yorkshire and the Humber     23   31\n\n\nTask 7 Compare between crime_rate with the pct_social_rented in constituency election dataset. What pattern you can learn from your boxplot?\n\nboxplot(pct_social_rented ~ crime_rate , data = pc_data)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploratory Data Analysis - UK Election</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html",
    "href": "labs/03.IntroductoryStatistics.html",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "",
    "text": "3.1 Overview\nWe have used Census data for the past two weeks. Census data aim to collect detailed information about every person and household in the UK, particularly on personal characteristics, household and housing, work and education, health, migration, and so on. The aim of such collection is to support plan public services and infrastructures.\nDifferent from census (which counts the entire population), in many cases, we need to use surveys to obtain timely, cost-effective and probably more specific in-depth information from people. Therefore, this week’s practical session will draw upon two survey datasets:\nWe will use these survey datasets to investigate the sample more thoroughly and employ statistical approaches to determine its representativeness of the population. We will use both numerical and categorical variables to calculate the sample mean, Standard Error and Confidence Intervals in different sample size.\nYou may wish to recap this week’s lecture: Lecture 03",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#overview",
    "href": "labs/03.IntroductoryStatistics.html#overview",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "",
    "text": "UK Family Resource Survey\nWorld Value Survey",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#prepare-your-working-environment",
    "href": "labs/03.IntroductoryStatistics.html#prepare-your-working-environment",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.2 Prepare your working environment",
    "text": "3.2 Prepare your working environment\n\nFor this Week 3 session, create a sub-folder called Week3 in your ENVS162 folder on your M-Drive. This is exactly the step we did in Week 1 and 2 and we will do this every week to Week 5.\nDownload this week’s practical datasets from Canvas Week 3: family_resource_survey.csv, world_value_suvey.csv and world_map.geojson. Save all these three datasets in the Week 3 folder you just created.\nOpen RStudio\nOpen a new R Script for your Week 3 work, rename it as Week3.R and save it in your newly created Week 3 folder, under M drive -&gt; ENVS162 folder.\nCheck whether there is any previous left dataframes in your RStudio in the upper-right side Environment pane. You can always use the to clear all the dataframes in your environment and make it all clean. For the same aim, you can click the icon , or you can run the below code:\n\n\nrm(list = ls())\n\nThis command and also the brush icon can both clear RStudio’s memory, removing any data objects that you have previously created.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#load-libraries-and-familiar-with-survey-data",
    "href": "labs/03.IntroductoryStatistics.html#load-libraries-and-familiar-with-survey-data",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.3 Load libraries and familiar with survey data",
    "text": "3.3 Load libraries and familiar with survey data\nExactly as what we have done for Week 1 and 2, before we start to do anything in R, we first need to load the essential libraries as all the functions/commands are packed in these libraries. For this week, we will still rely on tidyverse, tmap and sf.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nRecall that the data can be loaded into RStudio using the read.csv function:\n\n# use read.csv to load a CSV file\ndf &lt;- read.csv(\"family_resource_survey.csv\",stringsAsFactors = TRUE)\n\nThe stringsAsFactors = TRUE parameter tells R to read any character or text variables as classes or categories and not as just text.\nRecall what we should do to familiar ourselves with the dataset? We need to now how many rows and columns of the dataset, what are the variables, what types of these variables, and we may what to view the dataset for a quick scan?\nTherefore, we need these functions: dim() or ncol() and nrow(), names(), str(), and View().\n\n# know how many rows and columns\ndim(df)\n\n# know the names of the variables\nnames(df)\n\n# know types and examples of these variables\nstr(df)\n\n# open a view window to scan the dataset\nView(df)\n\n\nQuestion 1. How many people have been included in this survey?\n\nIn Week 2, we also know a simple way to help us get a quick look at the descriptive summary of all the variables:\n\n# summary all variables\nsummary(df)\n\nOkay, if for now you get yourself familiar with the dataset you are going to work on, let’s move on.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#sampling-of-numerical-variables",
    "href": "labs/03.IntroductoryStatistics.html#sampling-of-numerical-variables",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.4 Sampling of numerical variables",
    "text": "3.4 Sampling of numerical variables\nAs discussed in the lecture, the Standard Error (SE) of the sample mean indicates how much the estimated mean is expected to vary from sample to sample. Together, the sample mean and SE help us assess how accurately our sample reflects the true population mean. A smaller SE indicates our sample estimate is likely closer to the actual population value.\nFrom the steps of familiaring with the dataset, you should already know that there are 33,847 people included in this dataset. Recall our lecture, these 33,847 people should be selected by some sampling methods - random or stratified or multistaged - but in any case, they are a sample of the entire world population.\nHere, in this practical, let’s assume that our survey data is about a certain population. With such assumption, our population is the people what completed the survey. Thus in this section, we are pretending that the survey respondent are the population.\nWe can test the mean and standard deviation of the hh_income_net of the population using the functions mean() and sd(). In Week 2, we have learnt that the mean tells you the center of your numerical variable, and the standard deviation reveals how spread out the variable are around the mean.\n\n# calculate the mean value of the variable\nmean(df$hh_income_net)\n\n[1] 34485.51\n\n\n\n# calculate the standard deviation of the variable\nsd(df$hh_income_net)\n\n[1] 29379.1\n\n\nYou can examine the income_net distribution by creating a histogram as using the code below (you should be very competent of this from Week 2):\n\n# plot a histogram for the variable income_net in df\nhist(df$hh_income_net, breaks  = 100, \n    main = \"Histogram of a population\", \n    xlab = \"Height\", \n    col = \"wheat\",\n    border = \"grey\")\n\n\n\n\n\n\n\n\n\n3.4.1 Sample mean and Standard Error\nIn the example below, we will work with the hh_income_net variable and an initial sample of 10 people (observations).\nYou can assess a sample of 10 observations (survey respondents) taken from the population at random using the sample function.\n\n# Create a sample of the population\nsample_10  &lt;- slice_sample(df, n=10)\n\n\n# sample mean value\nmean(sample_10$hh_income_net)\n\n[1] 23618.4\n\n# sample standard deviation\nsd(sample_10$hh_income_net)\n\n[1] 13555.13\n\n\nYou will get a different values for these to the ones your friends get (and the ones created below), because you will have each extracted a different sample from the population - this is random sampling!\nYou should calculate the Standard Error (SE) of the sample mean by:\n\nIn R we run the code below:\n\n# sample SE\nsd(sample_10$hh_income_net)/sqrt(length(sample_10$hh_income_net))\n\n[1] 4286.508\n\n# or\nsd(sample_10$hh_income_net)/sqrt(nrow(sample_10))\n\n[1] 4286.508\n\n\n\nQuestion 2. Compare population mean, sample mean and SE mean from your results?\n\nNow,to see how using a larger sample improves the estimate, you will now repeat the previous example, generating a sample, determining the sample means and SEs, but using different sample sizes: 10, 50, 100, 200.\nComplete the below table for the comparison:\n\n\n\nRandom sample size\nSample mean\nStardard Error (SE) mean\n\n\n\n\n10\n\n\n\n\n50\n\n\n\n\n100\n\n\n\n\n200\n\n\n\n\n\nThe above should have given you the general idea that as sample size increases, the sample estimate of a population mean becomes more reliable. To show this clearly, the code below generates a trend line plot showing the impact of sample size on Standard Errors. As the sample gets larger the SE gets smaller as the sample more closely represents the true population.\n\n# create a vector of sample sizes\nX = seq(10, 200, 10)\n# check the result\nX\n\n [1]  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190\n[20] 200\n\n\n\n# create a vector of sample errors from these\n# an empty vector to be populated in the for loop below\nSE = vector()\n# now loop though each value in X\nfor (i in X){\n  # create a sample for the ith value in X\n  # set the seed - see the Info box below\n  set.seed(12)\n  sample.i = slice_sample(df, n=i)\n  # calculate the SE\n  se.i = sd(sample.i$hh_income_net)/sqrt(length(sample.i$hh_income_net))\n  # add to the SE vector\n  SE = append(SE, se.i)\n}\n# check the result\nSE\n\n [1] 11545.970  6139.880  4638.476  3893.327  3353.118  3067.783  2687.440\n [8]  2391.198  2261.770  2103.450  2114.780  1979.338  1915.625  1882.336\n[15]  1979.165  1878.847  1810.918  1725.824  1776.093  1705.949\n\n\nYou can then these plot these:\n\n# plot the SEs\nplot(x = X, y = SE,\n     pch = 19, col = \"darkgreen\",\n     xlab = \"Sample Size\", ylab = \"Standard Error\", \n     main = \"Impact of sample size on Standard Error\")\nlines(x = X, y = SE)\n\n\n\n\n\n\n\n\nSo…as sample size increases, the sample estimate of a population mean becomes more reliable. However, you should note that this was based on one-off sample generation and looking at the SE. In the next section you will apply confidence intervals to the sample in order to assess the robustness of the sample.\n\n\n3.4.2 Confidence Intervals\nThe idea of a confidence interval, CI, is a natural extension of the standard error. It allows us to define a level of confidence in our population parameter estimate gleaned from a sample:\n\nWe can use the qnorm function. qnorm(p) gives the value on the normal distribution such that the cumulative probability up to that value equals p. It is used at here to calculate the errors around the sample mean under an assumption of a normal distribution of the population (hence the norm bit of qnorm):\n\n# you have already created the sample with\n# sample_10  &lt;- slice_sample(df, n=10)\nm &lt;- mean(sample_10$hh_income_net)\nm\nstd &lt;- sd(sample_10$hh_income_net)\nn &lt;- length(sample_10$hh_income_net)\n\nerror &lt;- qnorm(0.975)*std/sqrt(n)\n\nlower.bound &lt;- m-error\nupper.bound &lt;- m+error\n\n\n# upper and lower bound\nupper.bound\nlower.bound\n\nThis is the 95% confidence interval, for the rest 2.5% in the lower tail and 2.5% in the upper tail. This is why you may find in the code, we use qnorm(0.975) as that refers to the cutoff points in two directions). Again, you may find your values subtly different due to random sample. But in my case, the mean household net income is 42,110 pound, and we are 95% confident that the mean household net income is between 22,842 pound and 61,376 pound. This can be a really fuzzy estimates, and it is because we only use 10 respondents from the survey and by using such as small sample size, the CI window would be very fuzzy.\nLet’s change the above code to a sample size of 1000:\n\n# change sample size from 10 to 1000 this time\nsample_1k  &lt;- slice_sample(df, n=1000)\nm &lt;- mean(sample_1k$hh_income_net)\nm\nstd &lt;- sd(sample_1k$hh_income_net)\nn &lt;- length(sample_1k$hh_income_net)\n\nerror &lt;- qnorm(0.975)*std/sqrt(n)\n\nlower.bound &lt;- m-error\nupper.bound &lt;- m+error\n\nlower.bound\nupper.bound\n\nNow you see that the sample mean has changed to 36,680, which indicates that the mean household net income is 36,680 pound, and we are 95% confident that the mean household net income is between 33,934 pound and 39,427 pound. Therefore, with this sample size as 1000 sample, the CI window is much narrow and the result is more solid.\nYou may try out if we use all the respondents as the sample, what is the result? In this case, we don’t need to slice_sample(), and the n should be n = nrow(df).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#sampling-of-categorical-variable",
    "href": "labs/03.IntroductoryStatistics.html#sampling-of-categorical-variable",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.5 Sampling of categorical variable",
    "text": "3.5 Sampling of categorical variable\nNow, we switch our focus to the categorical variable happiness in the dataframe. Similar to what we have done in last week, we first wish to have a descriptive summary of the categories and frequency of categories for categorical variables. We can use table() here:\n\n# categories and frequencies\ntable(df$happiness)\n\n\n  Fairly happy Fairly unhappy     Very happy   Very unhappy \n          8346           1552          15548           1079 \n\n\nYou may also remember how to create a table to display the frequency and proportion of each category:\n\n# use df to create a table, including frequency and percentage of each category\ndf %&gt;%\n  count(happiness) %&gt;%\n  mutate(pct = round(n / sum(n) * 100,1))\n\n       happiness     n  pct\n1   Fairly happy  8346 31.5\n2 Fairly unhappy  1552  5.9\n3     Very happy 15548 58.6\n4   Very unhappy  1079  4.1\n\n\nThe pct in the table, is the percentage of respondents in our Family Resource Survey sample who reported each level of happiness. In other words, they are the sample proportion for variable happiness in this dataset.\nMeanwhile, recall from Week 2 lecture, there are two types of categorical variables: nominal and ordinal. The key difference between them is whether the categories have a nature ordering. In this case, the happiness variable is ordinal, because its categories follow a meaningful order: Very unhappy, Fairly unhappy, Fairly happy, and Very happy.\nTherefore, as we wish to display the variable with an order, we use R function factor() to do so:\n\n#set the ordering of categories\ndf$happiness &lt;- factor(\n  df$happiness,\n  levels = c(\"Very unhappy\", \n             \"Fairly unhappy\", \n             \"Fairly happy\", \n             \"Very happy\"),\n  ordered = TRUE\n) \n\nIf we now re-run the code to generate the frequency table and proportions, you will notice that the categories appear in a more logical order, reflecting the ordinal structure of the variable.\nThis time, we label the proportion column as p̂ (p-hat) to emphasise that these values represent the sample proportions for each happiness category.\n\ndf %&gt;%\n  count(happiness) %&gt;%\n  mutate(p_hat = round(n / sum(n) * 100,1))\n\n       happiness     n p_hat\n1   Very unhappy  1079   4.1\n2 Fairly unhappy  1552   5.9\n3   Fairly happy  8346  31.5\n4     Very happy 15548  58.6\n\n\nWe use barplot() from last week to present the happiness variable distribution in the Family Resource Survey:\n\n#use bar plot to visualise the distribution of categorical variable\ntab=table(df$happiness)\nbarplot(tab)\n\n\n\n\n\n\n\n\n\nQuestion 3. What pattern you can identify from the barplot about happiness in this sample?\n\n\n3.5.1 Sample proportion and Standard Error of the proportion\nIn this section, we will focus on the Very happy category, we can check the sample mean of Very happy from the eariler work and the p_hat is 58.6%. This can also be calculated by mean():\n\n#calculate sample mean\np_hat = mean(df$happiness == \"Very happy\")\np_hat\n\n[1] 0.586164\n\n\nNow let’s calculate the Standard Error. Recap the equation of Standard Error for a proportion:\n\nSo, we need the sample size n first and then use n and p_hat to calculate the SE_p. If we select all the respondents in the dataset as the sample, then the n is equal to the nrow().\n\n#calculate Standard Error\nn = nrow(df)\nn\n\n[1] 26525\n\nSE_p &lt;- sqrt(p_hat * (1 - p_hat) / n)\nSE_p\n\n[1] 0.003024099\n\n\nTherefore, now we get the interpretation that the “Very happy” sample proportion typically varies by about 0.003 or 0.30 percentage.\nWe can also test n from 10, 100, 1000 to all to plot the SE_p and see how sample size increased will reduce the Standard Error of proportion:\n\nX = c(10, 100, 1000, 2000, 5000, 10000, n)\nSE_p = vector()\n# now loop though each value in X\nfor (i in X){\n  # create a sample for the ith value in X\n  # set the seed - see the Info box below\n  set.seed(20)\n  sample.i = slice_sample(df, n=i)\n  # calculate the sample mean of proportion\n  p_hat = mean(sample.i$happiness == \"Very happy\")\n  # calculate the SE of proportion\n  se.i = sqrt(p_hat * (1 - p_hat) / i)\n  # add to the SE vector\n  SE_p = append(SE_p, se.i)\n}\n# check the result\nSE_p\n\n[1] 0.158113883 0.049355851 0.015409607 0.010928626 0.006966181 0.004908920\n[7] 0.003024099\n\n\nWe can then these plot these:\n\n# plot the SEs\nplot(x = X, y = SE_p,\n     pch = 19, col = \"coral\",\n     xlab = \"Sample Size\", ylab = \"Standard Error of proportion\", \n     main = \"Impact of sample size on Standard Error of the proportion Very happy\")\nlines(x = X, y = SE_p)\n\n\n\n\n\n\n\n\nAgain, we know that with the increase of sample size n, the Standard Error decreases.\n\n\n3.5.2 Confidence Intervals for a proportion (95%)\nWe now calculate a 95% confidence interval for the proportion of respondents who reported being Very happy in our sample.\nTo do this, we use the full sample size n. Recall that for a proportion, CI for a proportion (95%) is\n\nThe critical value is again the qnorm(0.975) as we did for the numerical variable, if you qnorm(0.975) then you may find it is 1.96. The below R code help use to calculate the CI windows of sample mean, when sample size is n.\n\n#CI windows\np_hat = mean(df$happiness == \"Very happy\")\np_hat\n\n[1] 0.586164\n\nSE_p &lt;- sqrt(p_hat * (1 - p_hat) / n)\nerror_p &lt;- qnorm(0.975) * SE_p\nlower.bound &lt;- p_hat - error_p\nupper.bound &lt;- p_hat + error_p\nlower.bound\n\n[1] 0.5802369\n\nupper.bound\n\n[1] 0.5920911\n\n\nTherefore, the confidence interval can be interpreted as follows: in the current UK Family Resources Survey, the sample proportion of respondents reporting being Very happy is 58.6%. The 95% confidence interval indicates that we are 95% confident that the true population proportion lies between 55.6% and 61.7%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#compare-to-world-value-survey",
    "href": "labs/03.IntroductoryStatistics.html#compare-to-world-value-survey",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.6 Compare to World Value Survey",
    "text": "3.6 Compare to World Value Survey\nWe know that when the sample changes, the survey results may also differ. Now we turn to a different dataset — the World Values Survey — and repeat the same analytical process to examine how the happiness results compare.\n\n3.6.1 Happiness in different survey sample\nAs usual, we first load the World Value Survey data:\n\n# use read.csv to load a CSV file\ndat &lt;- read.csv(\"world_value_suvey.csv\",stringsAsFactors = TRUE)\n\nAgain, we wish to treat the happiness variable with its natural ordering:\n\n#set ordering\ndat$happiness &lt;- factor(\n  dat$happiness,\n  levels = c(\"Very unhappy\", \n             \"Fairly unhappy\", \n             \"Fairly happy\", \n             \"Very happy\"),\n  ordered = TRUE\n) \n\nWe also want to have a descriptive summary of the count in each category and compute the percentage distribution:\n\n# descriptive summary table\ndat %&gt;%\n  count(happiness) %&gt;%\n  mutate(p_hat2 = round(n / sum(n) * 100,1))\n\n       happiness     n p_hat2\n1   Very unhappy  2517    1.6\n2 Fairly unhappy 18032   11.8\n3   Fairly happy 87342   57.1\n4     Very happy 45118   29.5\n\n\nNot just as the table above, a barplot would better support the descriptive summary visually:\n\n# barplot visualise\ntab = table(dat$happiness)\ntab\n\n\n  Very unhappy Fairly unhappy   Fairly happy     Very happy \n          2517          18032          87342          45118 \n\nbarplot(tab)\n\n\n\n\n\n\n\n\nAs what we have done earlier, we want to focus on the Very happy category so we first calculate the sample proportion of respondents who are Very happy.\n\n#sample proportion of Very happy\np_hat2 = mean(dat$happiness == \"Very happy\")\np_hat2\n\n[1] 0.2948715\n\n\nTo estimate the Standard Error, we use the same coding, but this time, the sample size is called k and all the respondents are included:\n\n#Standard Error for the proportion of Very happy\nk = nrow(dat)\nk\n\n[1] 153009\n\nSE_p2 &lt;- sqrt(p_hat2 * (1 - p_hat2) / k)\nSE_p2\n\n[1] 0.001165714\n\n\nThe estimated proportion of respondents who are Very happy typically varies by about 0.0012 (or 0.12 percentage points) from sample to sample due to random sampling variation.\nAnd the Confidence Intervals:\n\np_hat2\n\n[1] 0.2948715\n\nSE_p2\n\n[1] 0.001165714\n\nerror_p2 &lt;- qnorm(0.975) * SE_p2\nlower.bound2 &lt;- p_hat2 - error_p2\nupper.bound2 &lt;- p_hat2 + error_p2\nlower.bound2\n\n[1] 0.2925868\n\nupper.bound2\n\n[1] 0.2971563\n\n\nThe sample proportion of respondents who are Very happy is estimated to be around 29.5%. The 95% confidence interval suggests that we are 95% confident that the true population proportion lies between 29.3% and 29.7%.\nCompare to our earlier results by using UK Family Resource Survey, the findings are quite lower. You may have your inference that this is because earlier respondents are all from the UK but now it is from all over the world. Therefore, let’s filter only UK respondents from this World Value Survey:\n\n# filter UK respondents from the whole survey sample\ndat_uk = dat %&gt;% filter(english_short_name == \"United Kingdom\")\n\n\n#calculate sample mean proportion for UK sample only\ndat_uk %&gt;%\n  count(happiness) %&gt;%\n  mutate(p_hat3 = round(n / sum(n) * 100,1))\n\n       happiness    n p_hat3\n1   Very unhappy   34    0.7\n2 Fairly unhappy  335    7.1\n3   Fairly happy 2572   54.6\n4     Very happy 1772   37.6\n\np_hat3 = mean(dat_uk$happiness == \"Very happy\")\np_hat3\n\n[1] 0.3759813\n\n\n\nQuestion 4. Compute the SE and CI for the UK sample only dataframe dat_uk. How you will interpret the results?\n\n\n\n3.6.2 Between regions and countries\nYou may have noticed that, even within the World Values Survey, the sample proportion of respondents reporting being Very happy in the UK is higher than the overall sample proportion.\nWe can now continue to explore how happiness varies across different regions and countries to see whether similar patterns emerge elsewhere.\nTo fulfill this task, we shall use the cross-tabulation method we did last week, by using table() between region_name and happiness:\n\n#cross-tabulate region vs happiness\ntable(dat$region_name, dat$happiness)\n\n          \n           Very unhappy Fairly unhappy Fairly happy Very happy\n  Africa            421           1627         4829       2645\n  Americas          287           2868        11541       9053\n  Asia              866           5468        26994      14992\n  Europe            927           7894        42287      17539\n  Oceania            16            175         1691        889\n\n\nYou may think, okay, but it is very hard to draw conclusions based on the counts. You are right. Let’s change the count table into a proportion one by using function prop.table().\nTo know the function, we can first ask R’s help:\n\n?prop.table\n\nOn your RStudio’s right-hand Help pane, we can learn that we can set parameter margin in to 1 to indicate the division should be done by row (each row sums up to as 1), and margin = 2 as the division by columns (each column sums up to 1).\nLet’s try both margin=1 and margin=2:\n\n#get the cross-tabulation result and present in proportion format, margin = 1\ntab_region_happy = table(dat$region_name, dat$happiness)\nprop.table(tab_region_happy, margin = 1) * 100\n\n          \n           Very unhappy Fairly unhappy Fairly happy Very happy\n  Africa      4.4213401     17.0867465   50.7141357 27.7777778\n  Americas    1.2084719     12.0762979   48.5957303 38.1194998\n  Asia        1.7922185     11.3162252   55.8650662 31.0264901\n  Europe      1.3503868     11.4994100   61.6006526 25.5495506\n  Oceania     0.5774089      6.3154096   61.0249008 32.0822808\n\nround(prop.table(tab_region_happy, margin = 1) * 100, 2)  #round(...,2) to keep only 2 digits for all the numbers\n\n          \n           Very unhappy Fairly unhappy Fairly happy Very happy\n  Africa           4.42          17.09        50.71      27.78\n  Americas         1.21          12.08        48.60      38.12\n  Asia             1.79          11.32        55.87      31.03\n  Europe           1.35          11.50        61.60      25.55\n  Oceania          0.58           6.32        61.02      32.08\n\n\nA vague calculation you may notice that each row (region) sums to 100 (as in the above codes, we multipled the proportion by 100 to express them as percentage). This means the table presents row percentages: within each region, the percentages show how respondents are distributed across the different levels of happiness. In other words, the table allows us to compare how the distribution of happiness varies across regions within the sample.\nIf we simply change margin = 1 to margin = 2, the results will be different because we are now calculating column percentages instead of row percentages.\n\nround(prop.table(tab_region_happy, margin = 2) * 100, 2)\n\n          \n           Very unhappy Fairly unhappy Fairly happy Very happy\n  Africa          16.73           9.02         5.53       5.86\n  Americas        11.40          15.91        13.21      20.07\n  Asia            34.41          30.32        30.91      33.23\n  Europe          36.83          43.78        48.42      38.87\n  Oceania          0.64           0.97         1.94       1.97\n\n\nWith margin = 2, each column sums to 100. This means we are looking at the distribution of regions within each happiness category.In other words, instead of asking: “Within each region, how is happiness distributed?” we are now asking: “Among respondents at each happiness level, how are they distributed across regions?” - This changes the interpretation entirely.\nNow it’s the time to check out the comparison among countries. Here we use variable english_short_name as the country to cross-tabulate happiness:\n\ntab_country_happy = table(dat$english_short_name, dat$happiness)\nround(prop.table(tab_country_happy, margin = 1) * 100, 1) #round(...,1) to keep only 2 digits for all the numbers\n\n\nQuestion 5. What conclusion you can draw from the findings? Change margin from 1 to 2, what new information you can learn from the results?\n\nAgain, in the following section, we want focus on the proportion of very happy in each country only, we can use two functions group_by() %&gt;% summarise() to fulfill this request:\n\n#group dat by the country name, in each group, calculate the mean proportion of happiness reported as Very happy\ndf_ctry_very_happy &lt;- dat %&gt;% \n  group_by(english_short_name) %&gt;% \n  summarise(p_hat = mean(happiness == \"Very happy\", na.rm = TRUE)) #if any rows include NA value, remove them\n\n\ndf_ctry_very_happy\n\nThis produces one row per country, with the sample proportion of respondents who are Very happy.\nWe can get the top 1 country by using function which.max(). This function returns the row number corresponding to the maximum value of p_hat. We can then use that row number to extract the full row from the dataframe: to get the row number of that country, and then ask R to return that row for us:\n\nwhich.max(df_ctry_very_happy$p_hat)\n\n[1] 44\n\ndf_ctry_very_happy[which.max(df_ctry_very_happy$p_hat),]\n\n# A tibble: 1 × 2\n  english_short_name p_hat\n  &lt;fct&gt;              &lt;dbl&gt;\n1 Kyrgyz Republic    0.677\n\n\nTo identify the top 10 happiest countries (based on the proportion of respondents who are Very happy), we can use arrange() function to sort the df_ctry_very_happy dataframe by p_hat.\nIf we place a minus sign - before p_hat (i.e. arrange(-p_hat)), the dataframe will be sorted in decreasing order, from highest to lowest proportion. Without the minus sign, the sorting will be in ascending order, from lowest to highest.\nYou can also type ?arrange in RStudio to view the official documentation and see additional details about how the function works.\nNow, top 10 Very happy:\n\ndf_ctry_very_happy %&gt;% arrange(-p_hat) %&gt;% head(10)\n\n# A tibble: 10 × 2\n   english_short_name p_hat\n   &lt;fct&gt;              &lt;dbl&gt;\n 1 Kyrgyz Republic    0.677\n 2 Tajikistan         0.626\n 3 Ecuador            0.618\n 4 Mexico             0.588\n 5 Colombia           0.574\n 6 Uzbekistan         0.519\n 7 Guatemala          0.511\n 8 Philippines        0.511\n 9 Puerto Rico        0.510\n10 Kenya              0.509\n\n\nbottom 10 Very happy:\n\ndf_ctry_very_happy %&gt;% arrange(p_hat) %&gt;% head(10)\n\n# A tibble: 10 × 2\n   english_short_name  p_hat\n   &lt;fct&gt;               &lt;dbl&gt;\n 1 South Korea        0.0410\n 2 Egypt              0.0644\n 3 Lithuania          0.0918\n 4 Hong Kong          0.111 \n 5 Morocco            0.126 \n 6 Latvia             0.132 \n 7 Slovakia           0.133 \n 8 Macao              0.145 \n 9 Russia             0.149 \n10 Greece             0.154 \n\n\n\n\n3.6.3 Map happiness around the world\nAs usual, let’s finish today’s practical by a map made by ourselves.\nFirst, load the world map boundaries from your working folder. Before producing the final map, we can generate a quick check map using qtm() to ensure the spatial data has loaded correctly. We also set tmap to interactive mode using tmap_mode(\"view\"), so the map can be explored dynamically (e.g., zooming and clicking on countries) before creating the final visualisation.\n\n#load the boundary dataset\nworld_map &lt;- st_read(\"world_map.geojson\")\n\nReading layer `world_map' from data source \n  `C:\\Users\\ziye\\Documents\\GitHub\\quant\\labs\\world_map.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 203 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -58.49861 xmax: 180 ymax: 83.6236\nGeodetic CRS:  WGS 84\n\n\n\n#set as interactive map\ntmap_mode(\"view\")\n#quick mapping\nqtm(world_map)\n\nClick on any country in the world map within the Viewer pane on the right-hand side of RStudio. You will notice that the variable english_short contains the country name.\nWe can use this variable as the key to join the spatial map data with our country-level happiness dataframe, since it corresponds to the english_short_name variable in our survey dataset.\nBefore join them, let’s first inspect the structure of both dataframe:\n\ndim(world_map)\n\n[1] 203  15\n\ndim(df_ctry_very_happy)\n\n[1] 91  2\n\n\nWe can learn that the world map has 203 countries, but our df_ctry_very_happy only have 91, this means that not all the countries from the world_map will be able to plot based on the p_hat from df_ctry_very_happy. As a result, some countries will have missing values (NA) for the happiness proportion and therefore will not be coloured according to p_hat in the final map.\nHowever, this is perfectly acceptable — those countries will simply appear with the default missing-value styling, while the countries included in our survey data will be shaded according to their estimated proportion of respondents who are Very happy.\nLet’s use function left_join() to join the two dataframes:\n\n#join our newly created dataframe df_ctry_very_happy to the world map by the shared columns\nworld_happiness = left_join(world_map, df_ctry_very_happy, by=c(\"english_short\"=\"english_short_name\") )\n\nA short code sentence would serve your first try on the new dataframe world_happiness. Don’t forget to click the  from your RStudio right-bottom Viewer pane for a full screen interactive map in your explorer:\n\n#View the map as an interview map\ntm_shape(world_happiness) +\n  tm_polygons(\"p_hat\")\n\nTo make it looking better, let’s change the color palette and breaks. Click the  from your RStudio right-bottom Viewer pane for a full screen interactive map - browse the map and interact with it to find how UK compare to other countries on this map? Any other findings interest you?\n\n#plot the map as a static map, with color palette as Yellow-Orange_Red, with break styles as jenks into 6 classes\n\nmap = tm_shape(world_happiness) +\n  tm_polygons(\"p_hat\",\n              fill.scale = tm_scale(values=\"YlOrRd\",\n                                style = \"jenks\",n=6))+\n   tm_layout(main.title = \"Mean proportion of Very happy in the World Value Survey\",\n            main.title.size=1.2,\n            frame = FALSE) + \n  tm_compass(position = c(\"right\", \"bottom\")) +\n  tm_scalebar(position = c(\"right\", \"bottom\")) \nmap\n\nAt last, we can use tmap_mode(\"plot\") to change the interactive map into a static one. Also we can save the map on your disk:\n\ntmap_mode(\"plot\")\nmap\ntmap_save(map,\"Week3 map.png\")\n\n\n\n\n\n\n\n\n\n\nCheck in your folder and you shall find the map made by yourself.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  },
  {
    "objectID": "labs/03.IntroductoryStatistics.html#formative-tasks",
    "href": "labs/03.IntroductoryStatistics.html#formative-tasks",
    "title": "3  Lab: Introductory Statistics - Happiness around the world",
    "section": "3.7 Formative Tasks",
    "text": "3.7 Formative Tasks\nTask 1 Use the income_gross variable from the UK Family Resource Survey, compare how increase sample size from 5000 to 20000 would affect the sample mean, standard error and CI of the income_gross. How to interpret the results?\nTask 2 Use the health variable from the UK Family Resource Survey, what is the sample mean proportion of Very Good health in the whole sample? The standard error and CI? How to interpret the results?\nTask 3 Use the health variable from the World Value Survey, compare your the sample mean proportion of Very good health with your findings in Task 2.\nTask 4 Use the trust_strangers variable from the World Value Survey, compare between different regions (variable use sub_region_name). Which region more likely to trust strangers completely?\nTask 5 Use the immigrant_impact variable from the World Value Survey, create a new dataframe to include the sample mean proportion of in each country/place who holds Very bad attitude of immigrant impact. Which 5 country/place has the lowest mean proportion in this sample?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Introductory Statistics - Happiness around the world</span>"
    ]
  }
]